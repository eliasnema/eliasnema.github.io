<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Elias Nema">
<meta name="dcterms.date" content="2020-09-05">

<title>ELIAS NEMA - üîÆ State of Recommendations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">ELIAS NEMA</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html">Talks</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">üîÆ State of Recommendations</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">recommendations</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Elias Nema </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 5, 2020</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#linkedin" id="toc-linkedin" class="nav-link" data-scroll-target="#linkedin"><img src="linkedin.png" width="35"> LinkedIn</a>
  <ul class="collapse">
  <li><a href="#linkedin-learning" id="toc-linkedin-learning" class="nav-link" data-scroll-target="#linkedin-learning">LinkedIn Learning</a></li>
  <li><a href="#linkedin-jobs" id="toc-linkedin-jobs" class="nav-link" data-scroll-target="#linkedin-jobs">LinkedIn Jobs</a></li>
  </ul></li>
  <li><a href="#avito" id="toc-avito" class="nav-link" data-scroll-target="#avito"><img src="avito.png" width="35"> Avito</a></li>
  <li><a href="#pinterest" id="toc-pinterest" class="nav-link" data-scroll-target="#pinterest"><img src="pinterest.png" width="35"> Pinterest</a></li>
  <li><a href="#coveo-coveo" id="toc-coveo-coveo" class="nav-link" data-scroll-target="#coveo-coveo"><img src="coveo.png" width="35" alt="coveo"> Coveo</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This is a lengthy piece covering 6 articles, 4 papers and demonstrating trends happening in the field of <strong>applied recommender systems</strong>. Cases below will be structured as follows:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid" data-tooltip-selector="#mermaid-tooltip-1">flowchart LR
  A(Overview) --&gt; B(Data)
  B --&gt; C(Model)
  C --&gt; D(Validation)
  D --&gt; E(Production)
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
<p>I prefer not to focus on the reported results because they are usually relative to the previous baselines and easily interpreted outside the context.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="reco.png" alt="Fig 1. Summary of Trends" width="500" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Fig 1. Summary of Trends</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<ul>
<li>Starting back in the days with a timeless classic of recsys ‚Äî matrix factorization, it‚Äôs now hard to find a system without some kind of <strong>neural nets, embedding spaces or sequential models.</strong></li>
<li>Serving has also moved to more <strong>real-time</strong> architectures and dynamic <strong>re-rankers.</strong> Not to mention that generally, the focus now is <strong>much more on the system as a whole</strong>, rather than just a modeling part.</li>
<li>Moving from relying mostly on explicit ratings to incorporating different <strong>implicit</strong> signals and giving them weights. Hence, also the <strong>regression‚Üíclassification</strong> shift.</li>
<li>On the <strong>item side</strong> pretty much every one converged to building embeddings to optimize for the product similarity.</li>
<li>On the <strong>user side,</strong> however, representations vary depending on the use case and business model.</li>
</ul>
</blockquote>
<p>But the truth is in details, so let‚Äôs dive in.</p>
</section>
<section id="linkedin" class="level2">
<h2 class="anchored" data-anchor-id="linkedin"><img src="linkedin.png" width="35"> LinkedIn</h2>
<section id="linkedin-learning" class="level3">
<h3 class="anchored" data-anchor-id="linkedin-learning">LinkedIn Learning</h3>
<p>We start with the LinkedIn Learning use case and their two-part article (<a href="https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-one">one</a>, <a href="https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-two">two</a>). The main goal for them is to <em>‚Äúsurface the most relevant and personalized course recommendations‚Äù</em>.</p>
<p>At LinkedIn, two main models are powering the offline (precalculated for each user daily) recommendations ‚Äî <strong>neural collaborative filtering</strong> and <strong>‚Äúresponse prediction‚Äù model.</strong></p>
<section id="linkedin-learning-collaborative-filtering-neural" class="level4">
<h4 class="anchored" data-anchor-id="linkedin-learning-collaborative-filtering-neural">LinkedIn Learning: Collaborative Filtering (Neural)</h4>
<p>Pros/cons of CF are well-known and LinkedIn didn‚Äôt escape them. Here is their recap:</p>
<ul>
<li>works better for core learners (members who are already active on the LinkedIn Learning platform)</li>
<li>focuses on recent interactions <em>(not a very common one, probably a property of their implementation)</em></li>
<li>diversified recommendations, no domain knowledge needed, however, cold start problems</li>
</ul>
<blockquote class="blockquote">
<p>I think recency and diversity are the most interesting points here.</p>
<ul>
<li><strong>Recency:</strong> generally, CF doesn‚Äôt give preference for the recent items unless implemented with some kind of time decay or position-aware component in the model.</li>
<li><strong>Diversity:</strong> in e-commerce, CF often <strong>reduces</strong> diversity because of the rich-get-richer effect. And here, probably, due to the fact that the course catalog is not that big and that every person can take any course, it actually increases it.</li>
</ul>
</blockquote>
<p><strong>üõ¢Data:</strong> course watch history data with a watch-time threshold (if a learner only watches the first three seconds of a course, it‚Äôs not included).</p>
<p><strong>üöóModel:</strong></p>
<ul>
<li>Computing learner and course embeddings in parallel. This is also known as a <em>two-tower architecture.</em></li>
<li>Log loss is used as an optimization function.</li>
<li>The modeling objective is to predict course watches using the past watches.</li>
<li>4 negative samples are taken for each positive one.</li>
<li>Holding the last interaction for each user for a test set ‚Äî a <strong>leave-one-out</strong> approach.</li>
</ul>
<blockquote class="blockquote">
<p>Two-tower architecture consists of two networks with fully connected layers in each becoming narrower with each consecutive layer.</p>
</blockquote>
<p><strong>üîÅValidation:</strong> A random sample of 100 items that user didn‚Äôt interact with is taken and the one hidden item (user‚Äôs last action) is ranked against these 100. Performance is judged by <strong>hit ratio and NDCG</strong> at 10.</p>
<blockquote class="blockquote">
<p>Interesting that input to the learner part is <strong>user/course co-occurrence matrix</strong>, while input to the course part is <strong>course/course similarity</strong> matrix.</p>
</blockquote>
<p><strong>üé¨Production:</strong> Generating top K courses for each user based on the similarity between embeddings. Calculating offline and storing in a key-value storage.</p>
</section>
<section id="linkedin-learning-response-prediction-model" class="level4">
<h4 class="anchored" data-anchor-id="linkedin-learning-response-prediction-model">LinkedIn Learning: Response Prediction Model</h4>
<p>Another offline model to compliment CF is so-called ‚ÄúResponse Prediction‚Äù. It takes into account user and course metadata as well as user‚Äôs actions.</p>
<blockquote class="blockquote">
<p>This algorithm typically performs better than CF for members with no/little previous engagement on LinkedIn Learning, as well as for new courses with few prior interactions.</p>
</blockquote>
<p><strong>üõ¢Data</strong>:</p>
<ul>
<li>User profile features (skills, industry, etc.)</li>
<li>Course metadata (difficulty, category, skills)</li>
<li>Historical explicit engagement (clicks, bookmarks, etc.) with the course watch time as an importance weight given to each click instance.</li>
</ul>
<blockquote class="blockquote">
<p>As a result, this importance weight helps to promote courses with higher watch times and creates a model that can optimize for course watches, not just clicks.</p>
</blockquote>
<p><strong>üöóModel</strong>: a fancy named ‚ÄúGeneralized Linear Mixture Model (GLMix)‚Äù is used here. In reality, it‚Äôs quite a simple (though hard computationally, <a href="https://www.kdd.org/kdd2016/papers/files/adf0562-zhangA.pdf">check the paper</a>) approach to express user‚Äôs probability to click via a sum of the three components: a global model, per-learner model, and per-course model.</p>
<blockquote class="blockquote">
<p>We are currently working on a model ensemble that can perform personalized blending of Response Prediction and Neural CF models to improve the overall performance on the final recommendation task. Secondly, we also plan to adopt Attention Models into our Neural CF framework for learner profiling, i.e., assigning attention weights to a learner‚Äôs course watch history to capture long term and short term interests in a more effective manner.</p>
</blockquote>
<p><strong>üîÅValidation:</strong> AUC for offline validation as well as click/apply rates for the online experiments.</p>
<p><strong>üé¨Production:</strong> offline two-stage ranking strategy.</p>
<ol type="1">
<li>storing courses and their features in Lucene, after, using features of users to generate 1000 candidates for each</li>
<li>ranking candidates using full GLMix model</li>
</ol>
</section>
</section>
<section id="linkedin-jobs" class="level3">
<h3 class="anchored" data-anchor-id="linkedin-jobs">LinkedIn Jobs</h3>
<p>In their <a href="https://engineering.linkedin.com/blog/2020/quality-matches-via-personalized-ai">next article,</a> LinkedIn shared some insights on their jobs recommendations <strong>(which can potentially be useful for any job-seeker).</strong> For example:</p>
<blockquote class="blockquote">
<p>Our analysis demonstrated that the majority of job applicants <strong>apply to at least 5 jobs</strong>, while the majority of <strong>job postings receive at least 10 applicants</strong>. This proves to result in enough data to train the personalization models.</p>
</blockquote>
<p><strong>Goal</strong>: to predict the probability of a positive recruiter action, conditional on a given member applying to a given job.</p>
<p><strong>üõ¢Data</strong>: how to identify if a signal is negative (have someone not replied because of lack of interest or because he is processing other candidates)? &gt; We make the negatives conclusive if no engagement is seen after 14 days. However, if a recruiter responds to other applications submitted later, we may infer the negative label earlier.</p>
<p>Now you know when to stop waiting for the recruiter‚Äôs response ‚è≥.</p>
<p><strong>üöóModel</strong>: here, the same model from the above (GLMix).</p>
<p>logit(P(positive response | member, job)) =<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>F_global(X_member, X_job) + F_member(X_job) + F_job(X_member)</code></p>
<blockquote class="blockquote">
<p>We used linear models for fm and fj, but one <strong>can use any model in the above formulation as long as the produced scores are calibrated to output log-odds</strong> (for example, a neural net). Usually, linear models are sufficient as per-member and per-job components, as individual members and individual jobs <strong>do not have enough interactions</strong> to train more complex non-linear models.</p>
</blockquote>
<p><strong>üîÅValidation</strong>: AUC and NDCG are used.</p>
<p><strong>üé¨Production</strong>:</p>
<ul>
<li>user/job models are retrained daily, with automated quality checks in place</li>
<li>global model his updated once every few weeks</li>
<li>initializing weights with existing values to reduce the training time</li>
</ul>
<p>You can see that features of your profile play a very important role when you are searching for a job, so if you are looking for something specific, <strong>make sure to include the relevant signals into your profile at least a couple of days before.</strong></p>
</section>
</section>
<section id="avito" class="level2">
<h2 class="anchored" data-anchor-id="avito"><img src="avito.png" width="35"> Avito</h2>
<p>A very <a href="https://habr.com/ru/company/avito/blog/491942/">important and thoughtful article</a> (in Russian) from our friends at Avito. And their approach takes a bit different direction. What if instead of learning personalized recommendations we‚Äôd try to optimize for the <strong>item similarity</strong>?</p>
<p><strong>üõ¢Data:</strong> pairs of ‚Äúsimilar‚Äù items.</p>
<p>How to define similars? Items that were ‚Äúcontacted‚Äù (the best proxy for the transaction in classifieds) by a user with a time threshold between actions (8h for Avito). This gives 1s as a label. How to get 0s? Negative sampling from the items that were active at the platform during the time of contact but were not selected by the user.</p>
<blockquote class="blockquote">
<p>Random sampling with a probability of an item being selected equals to a <strong>square root of a number of contacts that this item got</strong> (how popular the item was).</p>
</blockquote>
<p><strong>üöóModel:</strong> item features are fed into the embedding layers with a dropout and 2 linear layers on top. <strong>Item IDs are not included</strong> into item features to allow for model generalization.</p>
<blockquote class="blockquote">
<p>The last layer is tanh to transform the output into the [-1, 1] range and <strong>multiply by 128</strong> later to fit into the INT8 to save memory in serving.</p>
</blockquote>
<p>Calculating scores for the positive sample and 4000 (the more ‚Äî the better, constraint of the GPU memory) negative samples for each pair. Taking highest scores from the negatives (top 100 wrongly predicted items) and computing the log-loss.</p>
<blockquote class="blockquote">
<p>Learning only from the top 100 wrong predictions allows to <strong>save on training time without loosing the accuracy</strong>.</p>
</blockquote>
<p><strong>üîÅValidation:</strong> precision at 8 is calculated on a test set. Time-based split, with <strong>omitting 6 months</strong> of data between training and validation. This is done to see how model will behave after 6 months of not training.</p>
<p><strong>üé¨Production:</strong> The model is re-trained once per 6 months. This works fine because item IDs are not included into training so new items can be embedded into the space just by their features. So new items get‚Äôs represented in a space as soon as they are posted. Embeddings are stored in Sphinx search engine, which allows quite a fast vector search (p99 is under 200ms with 200k rpm, sharded by category).</p>
<p>Recommendations are used on both the item page (item-to-item) and the homefeed (user-to-item), with user-to-item generated from the similar items to the ones that user has seen recently.</p>
</section>
<section id="pinterest" class="level2">
<h2 class="anchored" data-anchor-id="pinterest"><img src="pinterest.png" width="35"> Pinterest</h2>
<p>And now ‚Äî Pinterest. Another very thoughtful and pragmatic <a href="https://medium.com/pinterest-engineering/pinnersage-multi-modal-user-embedding-framework-for-recommendations-at-pinterest-bfd116b49475">piece from them</a>.</p>
<p>Here as well, the general idea is to embed users/items into some space. However, having a single vector for user works bad ‚Äî no matter how good the network is it won‚Äôt be able to represent all the clusters of user‚Äôs interests. Another approach (the same as Avito takes above) is to represent <strong>a user via embeddings of items</strong> that he is interested in. But averaging of embeddings works bad for the longer-term user interests (e.g.&nbsp;<a href="https://miro.medium.com/max/1400/0*KB50oLyVlnzoGuen">paintings and shoes average to a salad</a>). The solution?</p>
<p>Run <strong>clustering</strong> on the user‚Äôs actions and take <strong>medoids</strong> (like centroids, but should be an existing item) from the most important clusters. Find similar items to those medoids.</p>
<p><strong>üõ¢Data:</strong> users‚Äô action pins from the last 90 days.</p>
<p><strong>üöóModel:</strong> the main model is quite a simple <a href="https://en.wikipedia.org/wiki/Ward%27s_method">Ward clustering</a> with the goal to produce different amount of clusters depending on a variety of items in the user‚Äôs history. A time decay average is used to assign importance to clusters.</p>
<p><strong>üîÅValidation:</strong> a very thorough approach to the evaluation process, highly recommend to check out more in the <a href="https://arxiv.org/pdf/2007.03634.pdf">paper</a>.</p>
<ul>
<li>Cluster user‚Äôs actions and rank clusters by importance.</li>
<li>Get 400 closest items to the medoids of the most important clusters.</li>
<li>Calculate <strong>relevance:</strong> the proportion of observed action pins that have high cosine similarity (‚â•0.8) with any recommended pin.</li>
<li>And <strong>recall:</strong> the proportion of action pins that are found in the recommendation set.</li>
<li>Test batches are calculated in the chronological order, day by day, simulating the production setup.</li>
</ul>
<p><strong>üé¨Production</strong>:</p>
<ul>
<li>Using HNSW for the approximate nearest neighbor search</li>
<li>Filtering out near-duplicates and lower quality pins</li>
<li>Using medoids allows saving on caching (no need to compute aNN all the time)</li>
</ul>
<p>Served using a classical <a href="https://en.wikipedia.org/wiki/Lambda_architecture">lambda architecture</a>:</p>
<blockquote class="blockquote">
<ol type="1">
<li>Daily Batch Inference: PinnerSage is run daily over the <strong>last 90 day actions</strong> of a user on a MapReduce cluster. The output of the daily inference job (list of medoids and their importance) are served online in key-value store.</li>
<li>Lightweight Online Inference: We collect the <strong>most recent 20 actions</strong> of each user on the latest day (after the last update to the entry in the key-value store) for online inference. PinnerSage uses a real-time event-based streaming service to consume action events and update the clusters initiated from the key-value store.</li>
</ol>
<p><strong>In practice, the system optimization plays a critical role in enabling the productionization of PinnerSage.</strong></p>
</blockquote>
</section>
<section id="coveo-coveo" class="level2">
<h2 class="anchored" data-anchor-id="coveo-coveo"><img src="coveo.png" width="35" alt="coveo"> Coveo</h2>
<p>Ok, item embeddings are great. But how about transfer learning for them? Wait, what? The thing is that many companies are actually multi-brand groups having more than one website.</p>
<p>Training embeddings for similar products in different shops will produce spaces which are not comparable. Is there a way to mitigate this? You bet there is.</p>
<p><strong>üõ¢Data:</strong> the best part is that you don‚Äôt need tons of data. All you need is data on how users interacted with products within sessions to build product spaces. After that, product features will come handy (text attributes, prices, images, etc.) to align spaces. Having <strong>cross-shop data</strong> is valuable later but not strictly necessary.</p>
<p><strong>üöóModel:</strong> product embeddings are trained using CBOW with negative sampling, by swapping the concept of words in a sentence with products in a browsing session. This is not the fanciest architecture for item embeddings ‚Äî check the Avito implementation above for a more sophisticated approach.</p>
<p>More interestingly though is a task to <strong>align product spaces</strong>. It‚Äôs different from aligning spaces for languages, mainly, because languanes are guaranteed to have similiar concepts, while for some products it‚Äôs not necesseraly true. Coveo has tried different models, but the general approach is:</p>
<ol type="1">
<li>Start with some unsupervised approach, such as pairing by item features, images, etc. This helps finding the initial mapping function.</li>
<li>Later, adjust the space alignment by learning from user interactions with the items in different spaces.</li>
</ol>
<p><strong>üîÅValidation:</strong> for the product embeddings model evaluation is done using the leave-one-out approach and by predicting the Nth interactions from the 0..N-1 items: embeddings are averaged for 0..N-1 items and the nearest neighbor search is done to predict the Nth item. NDCG@10 is used on the search result.</p>
<blockquote class="blockquote">
<p>Worth noting that this approach works for the short-lived sessions or specialized shops, while for sessions with multiple intents averaging might produce really weird results (see the Pinterest case above).</p>
</blockquote>
<p><strong>üé¨Production</strong>: they run 2 experiments. In both, the idea was to use an intent from one shop and aligned embeddings to</p>
<ul>
<li>predict the user‚Äôs next action</li>
<li>predict the best query in the search autocomplete</li>
</ul>
<p>Both experiments have proven the potential behind the approach and I‚Äôll be paying a close attention to the area of transfer learning for product spaces in the future.</p>
<hr>
<p><br> References:</p>
<ol type="1">
<li><a href="https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-one">LinkedIn‚Äôs Learning Recommendations Part 1</a></li>
<li><a href="https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-two">LinkedIn‚Äôs Learning Recommendations Part 2</a></li>
<li><a href="https://arxiv.org/pdf/1708.05031.pdf">LinkedIn‚Äôs Neural Collaborative Filtering</a></li>
<li><a href="https://www.kdd.org/kdd2016/papers/files/adf0562-zhangA.pdf">LinkedIn‚Äôs Generalized Linear Mixed Models</a></li>
<li><a href="https://engineering.linkedin.com/blog/2020/quality-matches-via-personalized-ai">LinkedIn Jobs‚Äô article</a></li>
<li>Avito‚Äôs <a href="https://habr.com/ru/company/avito/blog/491942/">article</a></li>
<li>Pinterest‚Äôs <a href="https://medium.com/pinterest-engineering/pinnersage-multi-modal-user-embedding-framework-for-recommendations-at-pinterest-bfd116b49475">article</a></li>
<li>Pinterest‚Äôs <a href="https://arxiv.org/pdf/2007.03634.pdf">paper</a></li>
<li>Coveo‚Äôs blog posts: <a href="https://blog.coveo.com/multi-brand-personalization-in-ecommerce/">one</a>, <a href="https://blog.coveo.com/clothes-in-space-real-time-personalization-in-less-than-100-lines-of-code/">two</a></li>
<li>Coveo‚Äôs <a href="https://blog.coveo.com/multi-brand-personalization-in-ecommerce/">paper</a></li>
<li><a href="https://developers.google.com/machine-learning/recommendation/dnn/softmax#can-you-use-item-features">Intro to recommendations</a> by Google</li>
</ol>


</section>

</main> <!-- /main -->
<!-- Begin Mailchimp Signup Form -->
  <div id="mc_embed_signup" style="padding-bottom: 1em; max-width: 400px;" class="ms-auto me-auto">
    <p style="font-weight: 600; margin-bottom: 0;">Subscribe</p>
    <span style="font-size: 0.9em;">Enjoy this blog? Get notified of new posts by email:</span>

    <form action="https://eliasnema.us17.list-manage.com/subscribe/post?u=968d81ab7c900cd9b4bbe76c6&amp;id=f416e619fd&amp;f_id=003f45e0f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_self">
        <div id="mc_embed_signup_scroll">
            <div class="input-group mt-1 mb-2">
                <input type="email" class="form-control" placeholder="Email Address" aria-label="Email Address" name="EMAIL" style="font-size: 0.8em; padding: .2em;">
                </div>            

	<div id="mce-responses" class="clear foot">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_968d81ab7c900cd9b4bbe76c6_f416e619fd" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot" style="display: flex; align-items: center; justify-content: center;">
                <input type="submit" value="Subscribe" name="subscribe" style="min-width: 150px; font-size: 0.8em;" id="mc-embedded-subscribe" class="button btn btn-light btn-sm ms-auto me-auto">
            </div>
        </div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/https:\/\/www\.eliasnema\.com/);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>