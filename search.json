[
  {
    "objectID": "posts/man_city_stats/city_stats.html",
    "href": "posts/man_city_stats/city_stats.html",
    "title": "Unaggregate your data, bust a quote and learn a thing about modern football",
    "section": "",
    "text": "Somehow I’ve spent the last week looking at Man City stats.\nIt all started with this quote in the book about the data and football that I’m reading now.\n\n\n\nSpoiler alert: this proved to be wrong. But it triggered interest from my side. Man City is one of the most recognisable sides, but do they play that short?"
  },
  {
    "objectID": "posts/man_city_stats/city_stats.html#a-common-pitfall",
    "href": "posts/man_city_stats/city_stats.html#a-common-pitfall",
    "title": "Unaggregate your data, bust a quote and learn a thing about modern football",
    "section": "A common pitfall",
    "text": "A common pitfall\nSo I wanted to quickly double-check the fact. Which, turned out, to be not easy at all. Available data is either not complete or too aggregated, there is a shortage of reliable publicly available sources. For example, the official premier league site says that MC has scored 13 goals from outside the box during the 2017/18 season, but there is no information about distributions (the quote mentions only the first half of the season).\nAnd here I fell in a very common mistake people make while working with data. It’s always tempting to look at aggregated data to see patterns and trends. However, often you need the exact opposite: take one sample, study it, see if it makes sense, see if it contains data that you expect.\n\n\nThis is a great example of a very common mistake people make when working with data. It’s always tempting to look at aggregated data to see patterns and trends. However, often you need the exact opposite: take one sample, study it, see if it makes sense, see if it contains data that you expect.\nIn fact, to prove the quote wrong I needed just a single example of a goal that City has scored from outside the box during that part of the season. So after searching for the best strikes of the season on YouTube I found that the quote didn’t hold up. Just take a look at this last-minute wonder-strike from Sterling.\nSo we’ve already proved the quote wrong simply by watching a couple of great football moments."
  },
  {
    "objectID": "posts/man_city_stats/city_stats.html#going-deeper",
    "href": "posts/man_city_stats/city_stats.html#going-deeper",
    "title": "Unaggregate your data, bust a quote and learn a thing about modern football",
    "section": "Going deeper",
    "text": "Going deeper\nBut now that we are in the topic, let’s dig deeper. The best website that I was able to find to look at more granular data is fbref. There I took statistics on every shot Man City did during their last 3 Premier League seasons. We also have a distance for each shot.\n\nVisualising\nHere and below I’ll be taking into consideration only first halves of the seasons. First, because the quote was about that. Second, this would be enough to see the tactical changes over time. Third, data for the 2019/20 season is incomplete anyways.\nLet’s start with trying to understand if there is any significant difference between seasons.\n\nThis visualisation is of little use here, but at least we can see some distinctions between seasons.\nLet’s quickly compute t-test statistics to get a numeric representation of it:\nseason 17/18 vs 18/19: Ttest_indResult(statistic=1.1348979627228202, pvalue=0.2568168035045629)\nseason 18/19 vs 19/20: Ttest_indResult(statistic=1.8185721011018878, pvalue=0.06943167359879912)\nseason 17/18 vs 19/20: Ttest_indResult(statistic=3.1215926762079387, pvalue=0.0018782472078475076)\n\n\nThis compounds to a huge difference from 2017 to 2019 seasons.\nIndeed, looks like there is a small but not very significant difference between 2017 and 2018, while a much bigger difference between 2018 and 2019. This compounds to a huge difference from 2017 to 2019 seasons.\nLet’s give this difference a better human interface.\n\n\nStacked graph\nOne way to look at distribution differences would be a stacked bar graph. This graph quickly falls apart when the number of dimension grows, but is perfect for our use case.\n\nIt’s not too detailed, but also very concise. And we clearly see that each season City is aiming to shoot more and from the closer range.\n\n\nDensity graphs\nIf we want to look even closer we can go with density graphs.\n\nWow, this tells a story now. A coincidence? I doubt that."
  },
  {
    "objectID": "posts/man_city_stats/city_stats.html#conclusion-learnings",
    "href": "posts/man_city_stats/city_stats.html#conclusion-learnings",
    "title": "Unaggregate your data, bust a quote and learn a thing about modern football",
    "section": "Conclusion & learnings",
    "text": "Conclusion & learnings\nI believe football evolves by learning more about itself. It’s a known fact that teams have started optimising for shooting from positions with higher chance of scoring, which translates into less long-ranged shots.\nEvolution of Man City from the season 2017 to 2019 shows that they indeed follow this trend of shooting from “better” positions. But does it make their game better? That’s the question we can’t objectively answer with data.\n\n\n\n\n\n\nSome concise takeaways for the data folks\n\n\n\n\nDon’t start with aggregated data. Firstly, make sure that single data points look reasonable.\nCompare distributions numerically using the t-test, Kolmogorov-Smirnov test, etc.\nAdd a visual story by bucketing:\n\nHistograms\nPivot histograms into a stacked chart for a different perspective\nGo for density estimations for more details"
  },
  {
    "objectID": "posts/man_city_stats/city_stats.html#code",
    "href": "posts/man_city_stats/city_stats.html#code",
    "title": "Unaggregate your data, bust a quote and learn a thing about modern football",
    "section": "Code",
    "text": "Code\n\nReading data\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ncity = pd.read_csv('man_city_17_18_19.csv')\n\n\n\n\nCode\ncity.head()\n\n\n\n\n\n\n  \n    \n      \n      Season\n      Matchday\n      Opponent\n      Minute\n      Player\n      Outcome\n      Distance\n      BodyPart\n      Notes\n      SCA1\n      SCA1Event\n      SCA2\n      SCA2Event\n    \n  \n  \n    \n      0\n      2017/18\n      1\n      Brighton & Hove Albion\n      4\n      Gabriel Jesus\n      Blocked\n      11\n      Right Foot\n      NaN\n      Fernandinho\n      Pass (Live)\n      Kyle Walker\n      Pass (Live)\n    \n    \n      1\n      2017/18\n      1\n      Brighton & Hove Albion\n      9\n      Danilo\n      Off Target\n      21\n      Right Foot\n      NaN\n      David Silva\n      Pass (Live)\n      Danilo\n      Pass (Live)\n    \n    \n      2\n      2017/18\n      1\n      Brighton & Hove Albion\n      14\n      Fernandinho\n      Off Target\n      10\n      Head\n      NaN\n      David Silva\n      Pass (Dead)\n      NaN\n      NaN\n    \n    \n      3\n      2017/18\n      1\n      Brighton & Hove Albion\n      17\n      Kevin De Bruyne\n      Saved\n      27\n      Right Foot\n      Free kick\n      David Silva\n      Fouled\n      Fernandinho\n      Pass (Live)\n    \n    \n      4\n      2017/18\n      1\n      Brighton & Hove Albion\n      32\n      Kevin De Bruyne\n      Blocked\n      28\n      Right Foot\n      Free kick\n      David Silva\n      Fouled\n      Danilo\n      Pass (Live)\n    \n  \n\n\n\n\n\n\nCode\nprint(city.Season.value_counts())\nprint(city.Matchday.nunique())\n\n\n2019/20    376\n2018/19    346\n2017/18    338\nName: Season, dtype: int64\n19\n\n\nData only for two halves of two seasons. Getting shots per searson.\n\n\nCode\ny17 = city.query(\"Season == '2017/18'\")['Distance']\ny18 = city.query(\"Season == '2018/19'\")['Distance']\ny19 = city.query(\"Season == '2019/20'\")['Distance']\n\n\n\n\nComparing two samples\n\n\nCode\nfrom matplotlib import pyplot as plt\nplt.style.use('Solarize_Light2')\n%config InlineBackend.figure_format = 'retina'\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\n\nCode\nsns.distplot(y17, 20, label='17/18')\nsns.distplot(y18, 20, label='18/19')\nsns.distplot(y19, 20, label='19/20')\nplt.legend()\n\n\n<matplotlib.legend.Legend at 0x1579b7f70>\n\n\n\n\n\n\n\nCode\ny17.mean(), y17.std()\n\n\n(16.701183431952664, 8.51091453670634)\n\n\n\n\nCode\ny18.mean(), y18.std()\n\n\n(15.95086705202312, 8.779779921952818)\n\n\n\n\nCode\ny19.mean(), y19.std()\n\n\n(14.872340425531915, 6.9627336279989995)\n\n\n\n\nCode\n# equal_var=False to perform welch's test instead, because of different variance and amount of sample\nprint(f'season 17/18 vs 18/19: {stats.ttest_ind(y17,y18, equal_var=False)}')\nprint(f'season 18/19 vs 19/20: {stats.ttest_ind(y18,y19, equal_var=False)}')\nprint(f'season 17/18 vs 19/20: {stats.ttest_ind(y17,y19, equal_var=False)}')\n\n\nseason 17/18 vs 18/19: Ttest_indResult(statistic=1.1348979627228202, pvalue=0.2568168035045629)\nseason 18/19 vs 19/20: Ttest_indResult(statistic=1.8185721011018878, pvalue=0.06943167359879913)\nseason 17/18 vs 19/20: Ttest_indResult(statistic=3.1215926762079387, pvalue=0.001878247207847508)\n\n\nLooks like there is a high chance of difference between them.\n\n\nDensity distribution\n\n\nCode\nkde17 = stats.gaussian_kde(y17)\nkde18 = stats.gaussian_kde(y18)\nkde19 = stats.gaussian_kde(y19)\ngrid = np.linspace(0,50, 51)\n\n\n\n\nCode\nf, axs = plt.subplots(3,1, figsize=(8,12))\n\naxs[0].plot(grid, kde17(grid), label=\"season 2017/18\")\naxs[0].plot(grid, kde18(grid), label=\"season 2018/19\")\naxs[0].plot(grid, kde18(grid)-kde17(grid), label=\"difference between 2018 and 2017\")\n\naxs[1].plot(grid, kde18(grid), label=\"season 2018/19\")\naxs[1].plot(grid, kde19(grid), label=\"season 2019/20\")\naxs[1].plot(grid, kde19(grid)-kde18(grid), label=\"difference between 2019 and 2018\")\n\n# axs[2].plot(grid, kde17(grid), label=\"season 2017/18\")\n# axs[2].plot(grid, kde19(grid), label=\"season 2019/20\")\naxs[2].plot(grid, kde19(grid)-kde17(grid), label=\"difference between 2019 and 2017\")\n\nfor ax in axs:\n    ax.set(ylabel='Density')\n    ax.legend()\n\naxs[2].set(xlabel='Shooting distance')\nplt.show()\n\n\n\n\n\n\n\nCode\nplot = plt.figure(figsize=(8,3))\nplt.plot(grid, kde18(grid), label=\"season 2018/19\", figure=plot)\nplt.plot(grid, kde19(grid), label=\"season 2019/20\", figure=plot)\nplt.plot(grid, kde19(grid)-kde18(grid), label=\"difference between 2019 and 2018\")\nplt.xlabel('Shooting distance')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nplot = plt.figure(figsize=(8,3))\nplt.plot(grid, kde17(grid), label=\"season 2017/18\", figure=plot)\nplt.plot(grid, kde19(grid), label=\"season 2019/20\", figure=plot)\nplt.plot(grid, kde19(grid)-kde17(grid), label=\"difference between 2019 and 2017\")\nplt.xlabel('Shooting distance')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nplot = plt.figure(figsize=(8,3))\nplt.plot(grid, kde18(grid)-kde17(grid), label=\"difference between 18 and 17\", figure=plot)\nplt.plot(grid, kde19(grid)-kde18(grid), label=\"difference between 19 and 18\")\n# plt.plot(grid, kde19(grid)-kde17(grid), label=\"difference between 19 and 17\")\nplt.xlabel('Shooting distance')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\n\nYou can see the difference after plotting estimated densities.\nManchester City has started shooting more from the 5-10 and 15-20 yards range. They shoot less from > 30 yards.\n\n\nCode\ndiff=plt.bar(range(5,51,5), \n             height=(np.histogram(y19,bins=10, range=(0,50))[0] - np.histogram(y17,bins=10, range=(0,50))[0])\n             ) \nplt.title(\"a-b\")\n\n\nText(0.5, 1.0, 'a-b')\n\n\n\n\n\nHere is the difference between histograms.\n\n\nCode\ndf=pd.DataFrame([\n        np.histogram(y17,5,(0,50))[0] / y17.count() * 100,\n        np.histogram(y18,5,(0,50))[0] / y18.count() * 100,\n        np.histogram(y19,5,(0,50))[0] / y19.count() * 100\n        ], index=['2017','2018','2019'], columns=['0-10','10-20','20-30','30-40','40-50'])\n\n\n\n\nCode\ncity['Distance bins'] =  pd.cut(city['Distance'], bins=[0,10,20,30,40,50])\n\n\n\n\nCode\ndf1 = city[['Season', 'Distance bins','Distance']].groupby(by=['Season','Distance bins'],).agg('count').unstack()\n\n\n\n\nCode\ndf1.columns = ['(0, 10]','(10, 20]','(20, 30]','(30, 40]','(40, 50]']\n\n\n\n\nCode\nax = df1.plot(kind='bar', stacked=True,width=.85, figsize=(4,4))\nax.set(ylabel='# of shots')\nax.legend(bbox_to_anchor=(1, 1.025))\n\nfor i, p in enumerate(ax.patches):\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy()\n    pct = height/df1.iloc[i//5].sum()\n    if pct > .04:\n        ax.annotate('{:.0%}'.format(pct), (x+.27, y + height/2 -5 ), color='white', fontsize=10,)\n\n\n\n\n\n\n\nCode\npd.DataFrame([(df1.loc['2017/18']/df1.loc['2017/18'].sum()).T,\n(df1.loc['2018/19']/df1.loc['2018/19'].sum()).T,\n(df1.loc['2019/20']/df1.loc['2019/20'].sum()).T])\n\n\n\n\n\n\n  \n    \n      \n      (0, 10]\n      (10, 20]\n      (20, 30]\n      (30, 40]\n      (40, 50]\n    \n  \n  \n    \n      2017/18\n      0.281065\n      0.375740\n      0.269231\n      0.071006\n      0.002959\n    \n    \n      2018/19\n      0.321739\n      0.385507\n      0.240580\n      0.049275\n      0.002899\n    \n    \n      2019/20\n      0.313830\n      0.430851\n      0.252660\n      0.002660\n      0.000000\n    \n  \n\n\n\n\n\n\nCode\ndf1\n\n\n\n\n\n\n  \n    \n      \n      (0, 10]\n      (10, 20]\n      (20, 30]\n      (30, 40]\n      (40, 50]\n    \n    \n      Season\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017/18\n      95\n      127\n      91\n      24\n      1\n    \n    \n      2018/19\n      111\n      133\n      83\n      17\n      1\n    \n    \n      2019/20\n      118\n      162\n      95\n      1\n      0"
  },
  {
    "objectID": "posts/state-of-reco/state-of-reco.html",
    "href": "posts/state-of-reco/state-of-reco.html",
    "title": "🔮 State of Recommendations",
    "section": "",
    "text": "This is a lengthy piece covering 6 articles, 4 papers and demonstrating trends happening in the field of applied recommender systems. Cases below will be structured as follows:\n\n\n\n\nflowchart LR\n  A(Overview) --> B(Data)\n  B --> C(Model)\n  C --> D(Validation)\n  D --> E(Production)\n\n\n\n\n\n\n\n\nI prefer not to focus on the reported results because they are usually relative to the previous baselines and easily interpreted outside the context.\n\n\n\nFig 1. Summary of Trends\n\n\n\n\nStarting back in the days with a timeless classic of recsys — matrix factorization, it’s now hard to find a system without some kind of neural nets, embedding spaces or sequential models.\nServing has also moved to more real-time architectures and dynamic re-rankers. Not to mention that generally, the focus now is much more on the system as a whole, rather than just a modeling part.\nMoving from relying mostly on explicit ratings to incorporating different implicit signals and giving them weights. Hence, also the regression→classification shift.\nOn the item side pretty much every one converged to building embeddings to optimize for the product similarity.\nOn the user side, however, representations vary depending on the use case and business model.\n\n\nBut the truth is in details, so let’s dive in."
  },
  {
    "objectID": "posts/state-of-reco/state-of-reco.html#linkedin",
    "href": "posts/state-of-reco/state-of-reco.html#linkedin",
    "title": "🔮 State of Recommendations",
    "section": " LinkedIn",
    "text": "LinkedIn\n\nLinkedIn Learning\nWe start with the LinkedIn Learning use case and their two-part article (one, two). The main goal for them is to “surface the most relevant and personalized course recommendations”.\nAt LinkedIn, two main models are powering the offline (precalculated for each user daily) recommendations — neural collaborative filtering and “response prediction” model.\n\nLinkedIn Learning: Collaborative Filtering (Neural)\nPros/cons of CF are well-known and LinkedIn didn’t escape them. Here is their recap:\n\nworks better for core learners (members who are already active on the LinkedIn Learning platform)\nfocuses on recent interactions (not a very common one, probably a property of their implementation)\ndiversified recommendations, no domain knowledge needed, however, cold start problems\n\n\nI think recency and diversity are the most interesting points here.\n\nRecency: generally, CF doesn’t give preference for the recent items unless implemented with some kind of time decay or position-aware component in the model.\nDiversity: in e-commerce, CF often reduces diversity because of the rich-get-richer effect. And here, probably, due to the fact that the course catalog is not that big and that every person can take any course, it actually increases it.\n\n\n🛢Data: course watch history data with a watch-time threshold (if a learner only watches the first three seconds of a course, it’s not included).\n🚗Model:\n\nComputing learner and course embeddings in parallel. This is also known as a two-tower architecture.\nLog loss is used as an optimization function.\nThe modeling objective is to predict course watches using the past watches.\n4 negative samples are taken for each positive one.\nHolding the last interaction for each user for a test set — a leave-one-out approach.\n\n\nTwo-tower architecture consists of two networks with fully connected layers in each becoming narrower with each consecutive layer.\n\n🔁Validation: A random sample of 100 items that user didn’t interact with is taken and the one hidden item (user’s last action) is ranked against these 100. Performance is judged by hit ratio and NDCG at 10.\n\nInteresting that input to the learner part is user/course co-occurrence matrix, while input to the course part is course/course similarity matrix.\n\n🎬Production: Generating top K courses for each user based on the similarity between embeddings. Calculating offline and storing in a key-value storage.\n\n\nLinkedIn Learning: Response Prediction Model\nAnother offline model to compliment CF is so-called “Response Prediction”. It takes into account user and course metadata as well as user’s actions.\n\nThis algorithm typically performs better than CF for members with no/little previous engagement on LinkedIn Learning, as well as for new courses with few prior interactions.\n\n🛢Data:\n\nUser profile features (skills, industry, etc.)\nCourse metadata (difficulty, category, skills)\nHistorical explicit engagement (clicks, bookmarks, etc.) with the course watch time as an importance weight given to each click instance.\n\n\nAs a result, this importance weight helps to promote courses with higher watch times and creates a model that can optimize for course watches, not just clicks.\n\n🚗Model: a fancy named “Generalized Linear Mixture Model (GLMix)” is used here. In reality, it’s quite a simple (though hard computationally, check the paper) approach to express user’s probability to click via a sum of the three components: a global model, per-learner model, and per-course model.\n\nWe are currently working on a model ensemble that can perform personalized blending of Response Prediction and Neural CF models to improve the overall performance on the final recommendation task. Secondly, we also plan to adopt Attention Models into our Neural CF framework for learner profiling, i.e., assigning attention weights to a learner’s course watch history to capture long term and short term interests in a more effective manner.\n\n🔁Validation: AUC for offline validation as well as click/apply rates for the online experiments.\n🎬Production: offline two-stage ranking strategy.\n\nstoring courses and their features in Lucene, after, using features of users to generate 1000 candidates for each\nranking candidates using full GLMix model\n\n\n\n\nLinkedIn Jobs\nIn their next article, LinkedIn shared some insights on their jobs recommendations (which can potentially be useful for any job-seeker). For example:\n\nOur analysis demonstrated that the majority of job applicants apply to at least 5 jobs, while the majority of job postings receive at least 10 applicants. This proves to result in enough data to train the personalization models.\n\nGoal: to predict the probability of a positive recruiter action, conditional on a given member applying to a given job.\n🛢Data: how to identify if a signal is negative (have someone not replied because of lack of interest or because he is processing other candidates)? > We make the negatives conclusive if no engagement is seen after 14 days. However, if a recruiter responds to other applications submitted later, we may infer the negative label earlier.\nNow you know when to stop waiting for the recruiter’s response ⏳.\n🚗Model: here, the same model from the above (GLMix).\nlogit(P(positive response | member, job)) =\n                      F_global(X_member, X_job) + F_member(X_job) + F_job(X_member)\n\nWe used linear models for fm and fj, but one can use any model in the above formulation as long as the produced scores are calibrated to output log-odds (for example, a neural net). Usually, linear models are sufficient as per-member and per-job components, as individual members and individual jobs do not have enough interactions to train more complex non-linear models.\n\n🔁Validation: AUC and NDCG are used.\n🎬Production:\n\nuser/job models are retrained daily, with automated quality checks in place\nglobal model his updated once every few weeks\ninitializing weights with existing values to reduce the training time\n\nYou can see that features of your profile play a very important role when you are searching for a job, so if you are looking for something specific, make sure to include the relevant signals into your profile at least a couple of days before."
  },
  {
    "objectID": "posts/state-of-reco/state-of-reco.html#avito",
    "href": "posts/state-of-reco/state-of-reco.html#avito",
    "title": "🔮 State of Recommendations",
    "section": " Avito",
    "text": "Avito\nA very important and thoughtful article (in Russian) from our friends at Avito. And their approach takes a bit different direction. What if instead of learning personalized recommendations we’d try to optimize for the item similarity?\n🛢Data: pairs of “similar” items.\nHow to define similars? Items that were “contacted” (the best proxy for the transaction in classifieds) by a user with a time threshold between actions (8h for Avito). This gives 1s as a label. How to get 0s? Negative sampling from the items that were active at the platform during the time of contact but were not selected by the user.\n\nRandom sampling with a probability of an item being selected equals to a square root of a number of contacts that this item got (how popular the item was).\n\n🚗Model: item features are fed into the embedding layers with a dropout and 2 linear layers on top. Item IDs are not included into item features to allow for model generalization.\n\nThe last layer is tanh to transform the output into the [-1, 1] range and multiply by 128 later to fit into the INT8 to save memory in serving.\n\nCalculating scores for the positive sample and 4000 (the more — the better, constraint of the GPU memory) negative samples for each pair. Taking highest scores from the negatives (top 100 wrongly predicted items) and computing the log-loss.\n\nLearning only from the top 100 wrong predictions allows to save on training time without loosing the accuracy.\n\n🔁Validation: precision at 8 is calculated on a test set. Time-based split, with omitting 6 months of data between training and validation. This is done to see how model will behave after 6 months of not training.\n🎬Production: The model is re-trained once per 6 months. This works fine because item IDs are not included into training so new items can be embedded into the space just by their features. So new items get’s represented in a space as soon as they are posted. Embeddings are stored in Sphinx search engine, which allows quite a fast vector search (p99 is under 200ms with 200k rpm, sharded by category).\nRecommendations are used on both the item page (item-to-item) and the homefeed (user-to-item), with user-to-item generated from the similar items to the ones that user has seen recently."
  },
  {
    "objectID": "posts/state-of-reco/state-of-reco.html#pinterest",
    "href": "posts/state-of-reco/state-of-reco.html#pinterest",
    "title": "🔮 State of Recommendations",
    "section": " Pinterest",
    "text": "Pinterest\nAnd now — Pinterest. Another very thoughtful and pragmatic piece from them.\nHere as well, the general idea is to embed users/items into some space. However, having a single vector for user works bad — no matter how good the network is it won’t be able to represent all the clusters of user’s interests. Another approach (the same as Avito takes above) is to represent a user via embeddings of items that he is interested in. But averaging of embeddings works bad for the longer-term user interests (e.g. paintings and shoes average to a salad). The solution?\nRun clustering on the user’s actions and take medoids (like centroids, but should be an existing item) from the most important clusters. Find similar items to those medoids.\n🛢Data: users’ action pins from the last 90 days.\n🚗Model: the main model is quite a simple Ward clustering with the goal to produce different amount of clusters depending on a variety of items in the user’s history. A time decay average is used to assign importance to clusters.\n🔁Validation: a very thorough approach to the evaluation process, highly recommend to check out more in the paper.\n\nCluster user’s actions and rank clusters by importance.\nGet 400 closest items to the medoids of the most important clusters.\nCalculate relevance: the proportion of observed action pins that have high cosine similarity (≥0.8) with any recommended pin.\nAnd recall: the proportion of action pins that are found in the recommendation set.\nTest batches are calculated in the chronological order, day by day, simulating the production setup.\n\n🎬Production:\n\nUsing HNSW for the approximate nearest neighbor search\nFiltering out near-duplicates and lower quality pins\nUsing medoids allows saving on caching (no need to compute aNN all the time)\n\nServed using a classical lambda architecture:\n\n\nDaily Batch Inference: PinnerSage is run daily over the last 90 day actions of a user on a MapReduce cluster. The output of the daily inference job (list of medoids and their importance) are served online in key-value store.\nLightweight Online Inference: We collect the most recent 20 actions of each user on the latest day (after the last update to the entry in the key-value store) for online inference. PinnerSage uses a real-time event-based streaming service to consume action events and update the clusters initiated from the key-value store.\n\nIn practice, the system optimization plays a critical role in enabling the productionization of PinnerSage."
  },
  {
    "objectID": "posts/state-of-reco/state-of-reco.html#coveo-coveo",
    "href": "posts/state-of-reco/state-of-reco.html#coveo-coveo",
    "title": "🔮 State of Recommendations",
    "section": " Coveo",
    "text": "Coveo\nOk, item embeddings are great. But how about transfer learning for them? Wait, what? The thing is that many companies are actually multi-brand groups having more than one website.\nTraining embeddings for similar products in different shops will produce spaces which are not comparable. Is there a way to mitigate this? You bet there is.\n🛢Data: the best part is that you don’t need tons of data. All you need is data on how users interacted with products within sessions to build product spaces. After that, product features will come handy (text attributes, prices, images, etc.) to align spaces. Having cross-shop data is valuable later but not strictly necessary.\n🚗Model: product embeddings are trained using CBOW with negative sampling, by swapping the concept of words in a sentence with products in a browsing session. This is not the fanciest architecture for item embeddings — check the Avito implementation above for a more sophisticated approach.\nMore interestingly though is a task to align product spaces. It’s different from aligning spaces for languages, mainly, because languanes are guaranteed to have similiar concepts, while for some products it’s not necesseraly true. Coveo has tried different models, but the general approach is:\n\nStart with some unsupervised approach, such as pairing by item features, images, etc. This helps finding the initial mapping function.\nLater, adjust the space alignment by learning from user interactions with the items in different spaces.\n\n🔁Validation: for the product embeddings model evaluation is done using the leave-one-out approach and by predicting the Nth interactions from the 0..N-1 items: embeddings are averaged for 0..N-1 items and the nearest neighbor search is done to predict the Nth item. NDCG@10 is used on the search result.\n\nWorth noting that this approach works for the short-lived sessions or specialized shops, while for sessions with multiple intents averaging might produce really weird results (see the Pinterest case above).\n\n🎬Production: they run 2 experiments. In both, the idea was to use an intent from one shop and aligned embeddings to\n\npredict the user’s next action\npredict the best query in the search autocomplete\n\nBoth experiments have proven the potential behind the approach and I’ll be paying a close attention to the area of transfer learning for product spaces in the future.\n\n References:\n\nLinkedIn’s Learning Recommendations Part 1\nLinkedIn’s Learning Recommendations Part 2\nLinkedIn’s Neural Collaborative Filtering\nLinkedIn’s Generalized Linear Mixed Models\nLinkedIn Jobs’ article\nAvito’s article\nPinterest’s article\nPinterest’s paper\nCoveo’s blog posts: one, two\nCoveo’s paper\nIntro to recommendations by Google"
  },
  {
    "objectID": "posts/hypothesize/hypothesize.html",
    "href": "posts/hypothesize/hypothesize.html",
    "title": "Hypothesize your way into better products",
    "section": "",
    "text": "How do you quickly test ideas in your product? How do you include analytics in your day-to-day development process? How do you embrace failure and stop investing in something that’s not working?\n\n\n\nWhen really zoomed out, a common workflow in engineering teams looks like this: “Idea -> Implementation.” Work is usually done in short cycles to provide fast feedback loops. But generally, you take an idea, and you implement it.\nThis comes with two dangerous assumptions:\n\nthat you already know what to build; and\nthat everything that you start building needs to be built up to some final readiness.\n\nWhile these might have been fairly safe assumptions in some industries some years ago, it’s hard to find anything further from the truth today. Products are rapidly evolving, becoming more complex, becoming smarter. User expectations are growing at an even faster pace. It’s close to impossible to predict whether a new feature will have an impact just by relying on your gut feeling."
  },
  {
    "objectID": "posts/hypothesize/hypothesize.html#hedge-your-risks",
    "href": "posts/hypothesize/hypothesize.html#hedge-your-risks",
    "title": "Hypothesize your way into better products",
    "section": "Hedge your risks",
    "text": "Hedge your risks\nI’ve been working in product teams whose solutions rely heavily on data (recommendations, personalization) so that outcomes are not possible to predict (in recommenders, even offline metrics cannot give any guarantees) and iterations take a long time. In such setups, making a wrong assumption and sticking to it till the end brings the worst possible return on investments. There is also a lack of common ground in scope definitions between team members. Often, we just wanted to quickly test the feasibility of an idea, but it turned into a multi-month project that ended up being a disappointment for everyone.\nIt was clear that we needed a common language among engineering, data and product to save us from making costly bids while maximizing learnings. So we came up with a workflow that allowed us to speak the same language and that specified what the focus should be at each stage of the project."
  },
  {
    "objectID": "posts/hypothesize/hypothesize.html#a-common-dictionary",
    "href": "posts/hypothesize/hypothesize.html#a-common-dictionary",
    "title": "Hypothesize your way into better products",
    "section": "A common dictionary",
    "text": "A common dictionary\n\nThe flow starts with the hypothesis. This is a small but important difference. A hypothesis, by definition, is something that doesn’t have certainty in itself. It can be right; it can be wrong.\nImplementation becomes three separate steps:\n\nPrototype. Here you do more analysis, do some research (by this I mean just google whether you can already reuse something) and build a minimal prototype to test the hypothesis. It might require coding, but even that’s not strictly necessary. You cut all the corners to be able to test it faster.\nExperiment. Only if the prototype looks promising, start thinking about designing an experiment using it. Again, you aim at testing to collect feedback from real users as fast as possible. Sometimes, even testing it on your colleagues might give you enough information.\nProductionize. Only if the experiment ends up being positive and the ROI outweighs the potential implementation and maintenance costs, start investing in productionizing your prototype and embedding it natively into the product."
  },
  {
    "objectID": "posts/hypothesize/hypothesize.html#optimize-time-to-value",
    "href": "posts/hypothesize/hypothesize.html#optimize-time-to-value",
    "title": "Hypothesize your way into better products",
    "section": "Optimize time to value",
    "text": "Optimize time to value\nIn the figure below, you can see that it’s possible to “short-circuit” the flow. And this, possibly, is the single most important part of it.\nIf, in the prototyping phase, either the additional analysis or the first draft shows that it doesn’t make sense to move forward? Go back and adjust your hypothesis with the new learnings. If, after the experiment phase, there is no expected uplift – no worries, just go back and tune your hypothesis.\n\nAll of these allows us to save costs, possibly in the most expensive parts, and to improve time to value. Time to value is a commonly optimized metric that measures the length of time necessary to realize the benefits of the solution.\nAnd your actual savings will be even greater than you see in the figure above, because each of the following components is potentially bigger in time investment than the previous one. Of course, it will strongly depend on the change you’re making, but generally it’s easier to create a prototype than to productionize it."
  },
  {
    "objectID": "posts/hypothesize/hypothesize.html#conclusion",
    "href": "posts/hypothesize/hypothesize.html#conclusion",
    "title": "Hypothesize your way into better products",
    "section": "Conclusion",
    "text": "Conclusion\nThis framework doesn’t command you HOW to build your product. Instead, it guides your thinking into WHAT is the most meaningful thing to work on at any particular moment. This allows you to hedge product risks while maximizing learnings, build a common dictionary within a team and optimize time to value.\nSince it serves only as guidance, it can also work together with scrum, radical focus or any other process you’re using. No need to break what’s already working.\nI’m going into more details about how to generate hypotheses and about each of the steps in the book I’m working now. It’s called Hypothesize! and you can pre-order it now at half price.\nNote that you will only get charged after the release date."
  },
  {
    "objectID": "posts/arsenal/arsenal.html",
    "href": "posts/arsenal/arsenal.html",
    "title": "Makeover Monday W30: Arsenal FC stats 2018/19",
    "section": "",
    "text": "I’ve always wanted to try the makeover monday since I learned about the project. But busy being busy, you know.\nLast week, however, two things happened:\n\nArsenal’s last season stats were posted as a weekly project (and I love football).\nI accidentally noticed that.\n\nSo it was decided - I’m creating a visualisation.\n\n\nCode\n# Imports\nimport pandas as pd\nimport plotly.express as px\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\npd.set_option('display.max_columns', None)\n\ndf = pd.read_excel('Arsenal Player Stats 2018-19.xlsx')"
  },
  {
    "objectID": "posts/arsenal/arsenal.html#eda",
    "href": "posts/arsenal/arsenal.html#eda",
    "title": "Makeover Monday W30: Arsenal FC stats 2018/19",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nprint(f'columns: {df.columns}')\n\n\ncolumns: Index(['Rank', 'Player', 'Nationality', 'Metric', 'Stat'], dtype='object')\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Rank\n      Player\n      Nationality\n      Metric\n      Stat\n    \n  \n  \n    \n      0\n      1\n      Pierre-Emerick Aubameyang\n      Gabon\n      Appearances\n      36\n    \n    \n      1\n      2\n      Alex Iwobi\n      Nigeria\n      Appearances\n      35\n    \n    \n      2\n      2\n      Alexandre Lacazette\n      France\n      Appearances\n      35\n    \n    \n      3\n      4\n      Lucas Torreira\n      Uruguay\n      Appearances\n      34\n    \n    \n      4\n      5\n      Matteo Guendouzi\n      France\n      Appearances\n      33\n    \n  \n\n\n\n\nYou can see that data is stored as rows per player/metric pair. Let’s pivot to get the player’s statistics in each row.\n\n\nCode\ntraining = df.pivot(index='Player', columns='Metric', values='Stat')\ntraining.head()\n\n\n\n\n\n\n  \n    \n      Metric\n      Appearances\n      Assists\n      Big Chances Missed\n      Clearances\n      Clearances Off Line\n      Dispossessed\n      Fouls\n      Goals\n      High Claim\n      Hit Woodwork\n      Minutes Played\n      Offsides\n      Own Goal\n      Passes\n      Punches\n      Red Cards\n      Saves\n      Shots\n      Tackles\n      Touches\n      Yellow Cards\n    \n    \n      Player\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Aaron Ramsey\n      28\n      6\n      2\n      3\n      0\n      27\n      21\n      4\n      0\n      0\n      1328\n      4\n      0\n      773\n      0\n      0\n      0\n      33\n      34\n      1029\n      0\n    \n    \n      Ainsley Maitland-Niles\n      16\n      1\n      0\n      11\n      1\n      17\n      8\n      1\n      0\n      0\n      986\n      1\n      0\n      451\n      0\n      1\n      0\n      5\n      33\n      782\n      1\n    \n    \n      Alex Iwobi\n      35\n      6\n      3\n      10\n      0\n      56\n      8\n      3\n      0\n      0\n      1972\n      2\n      0\n      951\n      0\n      0\n      0\n      35\n      28\n      1415\n      0\n    \n    \n      Alexandre Lacazette\n      35\n      8\n      13\n      29\n      0\n      61\n      51\n      13\n      0\n      1\n      2505\n      24\n      0\n      771\n      0\n      0\n      0\n      81\n      35\n      1313\n      2\n    \n    \n      Bernd Leno\n      32\n      0\n      0\n      32\n      0\n      1\n      0\n      0\n      10\n      0\n      2835\n      0\n      0\n      922\n      16\n      0\n      105\n      0\n      0\n      1276\n      0\n    \n  \n\n\n\n\n\nAdding Positions\nI wanted to cluster players together to see if there are any patterns and groups. Then realised - it doesn’t make a good visualisation: hey, look, these players have more Saves - they are probably goalkeepers. So I decided to add player field position to the dataset.\nWith this data, let’s look at performance by position. Maybe a radar chart can work for such kind of comparison?\nThere is a limited amount of axes on a graph for users to still make sense of it. How to choose metrics to display? There are obvious choices of course: saves for goalkeepers, tackles for defenders, goals for forwards. But how about common attributes such as passes or touches?\nTo identify potentially interesting data points, I started with a correlation plot. I’m looking for interesting patterns, i.e. metrics that are not linearly correlated with the minutes played.\n\n\nCode\nposition_lookup = {k:'' for k in training.index.tolist()}\n\nfor gk in ['Bernd Leno', 'Petr Cech']:\n    position_lookup[gk] = 'GK'\n\nfor df in [\n    'Carl Jenkinson','Héctor Bellerín','Laurent Koscielny','Rob Holding','Sead Kolasinac'\n    ,'Shkodran Mustafi','Sokratis','Stephan Lichtsteiner'\n]:\n    position_lookup[df] = 'DF'\n\nfor mf in [\n    'Aaron Ramsey','Ainsley Maitland-Niles','Denis Suárez','Granit Xhaka','Henrikh Mkhitaryan'\n    ,'Joe Willock','Konstantinos Mavropanos','Lucas Torreira', 'Matteo Guendouzi','Mesut Özil'\n    ,'Mohamed Elneny','Nacho Monreal',\n]:\n    position_lookup[mf] = 'MF'\n\n\nfor fw in [\n    'Alex Iwobi','Alexandre Lacazette','Bukayo Saka','Danny Welbeck','Eddie Nketiah','Pierre-Emerick Aubameyang'\n]:\n    position_lookup[fw] = 'FW'\n\ntraining['position'] = training.index.map(position_lookup)\n\n\n\n\nCode\ndata_per_position = training.groupby('position').sum()\ndata_per_position\n\n\n\n\n\n\n  \n    \n      Metric\n      Appearances\n      Assists\n      Big Chances Missed\n      Clearances\n      Clearances Off Line\n      Dispossessed\n      Fouls\n      Goals\n      High Claim\n      Hit Woodwork\n      Minutes Played\n      Offsides\n      Own Goal\n      Passes\n      Punches\n      Red Cards\n      Saves\n      Shots\n      Tackles\n      Touches\n      Yellow Cards\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      DF\n      143\n      12\n      2\n      459\n      1\n      64\n      143\n      6\n      0\n      0\n      11479\n      17\n      1\n      6968\n      0\n      0\n      0\n      64\n      201\n      9185\n      34\n    \n    \n      FW\n      120\n      20\n      39\n      59\n      0\n      156\n      76\n      40\n      0\n      5\n      7425\n      51\n      0\n      2488\n      0\n      0\n      0\n      217\n      88\n      3984\n      2\n    \n    \n      GK\n      39\n      0\n      0\n      39\n      0\n      1\n      0\n      0\n      16\n      0\n      3420\n      0\n      0\n      1146\n      18\n      0\n      133\n      0\n      1\n      1591\n      0\n    \n    \n      MF\n      229\n      20\n      9\n      205\n      2\n      188\n      193\n      23\n      0\n      3\n      15241\n      21\n      0\n      10203\n      0\n      2\n      0\n      186\n      319\n      13366\n      36\n    \n  \n\n\n\n\n\n\nCode\n# Common metrics\n\ntraining['passes_per_min_played'] = training['Passes'] / training['Minutes Played']\ntraining['passes_per_appearance'] = training['Passes'] / training['Appearances']\ntraining['min_played_per_apperance'] = training['Minutes Played'] / training['Appearances']\ntraining['touches_per_appearance'] = training['Touches'] / training['Appearances']\ntraining['touches_per_min_played'] = training['Touches'] / training['Minutes Played']\n\n# GK metrics\ntraining['high_claim_per_min_played'] = training['High Claim'] / training['Minutes Played']\ntraining['punches_per_min_played'] = training['Punches'] / training['Minutes Played']\ntraining['saves_per_min_played'] = training['Saves'] / training['Minutes Played']\n\ntraining['high_claim_per_appearance'] = training['High Claim'] / training['Appearances']\ntraining['punches_per_appearance'] = training['Punches'] / training['Appearances']\ntraining['saves_per_appearance'] = training['Saves'] / training['Appearances']\n\n# DF metrics\ntraining['clearances_per_min_played'] = training['Clearances'] / training['Minutes Played']\ntraining['clearances_per_appearance'] = training['Clearances'] / training['Appearances']\ntraining['clearances_per_touch'] = training['Clearances'] / training['Touches']\ntraining['fouls_per_appearance'] = training['Fouls'] / training['Appearances']\ntraining['fouls_per_clearance'] = training['Fouls'] / training['Clearances']\ntraining['fouls_per_touch'] = training['Fouls'] / training['Touches']\ntraining['yc_per_min_played'] = training['Yellow Cards'] / training['Minutes Played']\ntraining['yc_per_tackle'] = training['Yellow Cards'] / training['Tackles']\ntraining['yc_per_foul'] = training['Yellow Cards'] / training['Fouls']\ntraining['tackles_per_appearance'] = training['Tackles'] / training['Appearances']\ntraining['tackles_per_min_played'] = training['Tackles'] / training['Minutes Played']\n\n# Midfield metrics\ntraining['assists_per_appearance'] = training['Assists'] / training['Appearances']\ntraining['assists_per_min_played'] = training['Assists'] / training['Minutes Played']\ntraining['assists_per_pass'] = training['Assists'] / training['Passes']\ntraining['assists_per_touch'] = training['Assists'] / training['Touches']\ntraining['dispossessed_per_appearance'] = training['Dispossessed'] / training['Appearances']\ntraining['dispossessed_per_min_played'] = training['Dispossessed'] / training['Minutes Played']\ntraining['dispossessed_per_pass'] = training['Dispossessed'] / training['Passes']\ntraining['dispossessed_per_touch'] = training['Dispossessed'] / training['Touches']\ntraining['fouls_per_appearance'] = training['Fouls'] / training['Appearances']\ntraining['fouls_per_clearance'] = training['Fouls'] / training['Clearances']\ntraining['fouls_per_min_played'] = training['Fouls'] / training['Minutes Played']\ntraining['fouls_per_touch'] = training['Fouls'] / training['Touches']\ntraining['goals_per_appearance'] = training['Goals'] / training['Appearances']\ntraining['goals_per_min_played'] = training['Goals'] / training['Minutes Played']\ntraining['goals_per_shot'] = training['Goals'] / training['Shots']\ntraining['goals_per_touch'] = training['Goals'] / training['Touches']\ntraining['passes_per_appearance'] = training['Passes'] / training['Appearances']\ntraining['passes_per_min_played'] = training['Passes'] / training['Minutes Played']\ntraining['passes_per_touches'] = training['Passes'] / training['Touches']\ntraining['shots_per_appearance'] = training['Shots'] / training['Appearances']\ntraining['shots_per_min_played'] = training['Shots'] / training['Minutes Played']\ntraining['shots_per_touch'] = training['Shots'] / training['Touches']\ntraining['tackles_per_appearance'] = training['Tackles'] / training['Appearances']\ntraining['tackles_per_min_played'] = training['Tackles'] / training['Minutes Played']\ntraining['tackles_per_touch'] = training['Tackles'] / training['Touches']\ntraining['tackles_per_clearance'] = training['Tackles'] / training['Clearances']\ntraining['touches_per_apperance'] = training['Touches'] / training['Appearances']\ntraining['touches_per_min_played'] = training['Touches'] / training['Minutes Played']\n\n# FW metrics\ntraining['assists_per_min_played'] = training['Assists'] / training['Minutes Played']\ntraining['missed_chances_per_shot'] = training['Big Chances Missed'] / training['Shots']\ntraining['missed_chances_per_min_played'] = training['Big Chances Missed'] / training['Minutes Played']\ntraining['goals_per_min_played'] = training['Goals'] / training['Minutes Played']\ntraining['offsides_per_min_played'] = training['Offsides'] / training['Minutes Played']\ntraining['shots_per_min_played'] = training['Shots']/ training['Minutes Played']\n\ntraining['assists_per_appearance'] = training['Assists'] / training['Appearances']\ntraining['missed_chances_per_appearance'] = training['Big Chances Missed'] / training['Appearances']\ntraining['goals_per_appearance'] = training['Goals'] / training['Appearances']\ntraining['offsides_per_appearance'] = training['Offsides'] / training['Appearances']\ntraining['shots_per_appearance'] = training['Shots']/ training['Appearances']\n\n\n\n\nCode\nax = scatter_matrix(\n    training.query(\"position == 'DF'\")[\n        [\n            'Clearances',\n            'Clearances Off Line',\n            'Fouls',\n            'Tackles',\n            'Red Cards',\n            'Yellow Cards',\n            'Touches',\n            'Minutes Played']\n        ],\n        figsize=(20,12))\n\n\n\n\n\nThere are some:\n\nbad choices (clearances off the line - only a couple of players have at least 1);\nnot so obvious but not interesting - the more you play the more your KPI is;\npotentially distinguishable KPIs.\n\nWhat would be the best way to put it on the graph?\n\n\nScaling\nIf we want to create player profiles, we cannot just say Cech made X saves while Leno made Y. We need to normalise it. The most precise would be to do it by minutes played. So tackles per minute played or passes per minute played.\nHaving these numbers, we can already iterate to find the best representation.\n\nRatios as-is\nThe good part here - the real values are displayed for the KPIs. We can see Mustafi is completing around 0.06 clearances per minute played, more than anyone else. The problem - it’s hard to have all the numbers at the same scale, so some metrics skew the scale towards one direction and so it becomes hard to see values for smaller values.\n\nMustafi - first in memes but also completing around 0.06 clearances and 0.025 tackles per minute, more than any other defender in a team.\n\n\n\nLog-scaling\nWe can try to play with axis scales to get a more equal distribution of numbers. As you can see now graphs are more equally distributed, but it’s a mess in readability.\n\n\nRelative scaling\nWe can divide values by the maximum for the metric at this position. So all players will get a percentage from the maximum for the position in a team. As for the example above: Mustafi will get 1 for the clearances, Sokratis around 0.8 from that and so on.\nPersonally, I like the last approach most, even though with it we are losing the interpretability.\n\n\n\n\n\n\n\nRatios\n\n\n\n\n\n\n\nLog-scaled\n\n\n\n\n\n\n\nMax-scaled\n\n\n\n\nFigure 1: Different attempts at a radar chart"
  },
  {
    "objectID": "posts/arsenal/arsenal.html#by-position",
    "href": "posts/arsenal/arsenal.html#by-position",
    "title": "Makeover Monday W30: Arsenal FC stats 2018/19",
    "section": "By Position",
    "text": "By Position\n\nGoalkeepers\n\n\nCode\ngk = training.query(\"position == 'GK'\")[\n    ['high_claim_per_appearance', 'punches_per_appearance', 'saves_per_appearance'\n    ,'passes_per_appearance','clearances_per_appearance', 'Minutes Played']\n].rename(columns={'high_claim_per_appearance':'High Claims', 'punches_per_appearance': 'Punches', \n                 'saves_per_appearance':'Saves', 'passes_per_appearance': 'Passes',\n                  'clearances_per_appearance': 'Clearances'})\n\nmax_map = gk.max().to_dict()\ngk = pd.DataFrame(gk.unstack()).reset_index()\ngk.columns = ['kpi', 'name', 'value']\ngk['denom'] = gk.kpi.map(max_map)\ngk['value'] = gk.value / gk.denom\ngk.T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      kpi\n      High Claims\n      High Claims\n      Punches\n      Punches\n      Saves\n      Saves\n      Passes\n      Passes\n      Clearances\n      Clearances\n      Minutes Played\n      Minutes Played\n    \n    \n      name\n      Bernd Leno\n      Petr Cech\n      Bernd Leno\n      Petr Cech\n      Bernd Leno\n      Petr Cech\n      Bernd Leno\n      Petr Cech\n      Bernd Leno\n      Petr Cech\n      Bernd Leno\n      Petr Cech\n    \n    \n      value\n      0.364583\n      1.0\n      1.0\n      0.571429\n      0.820312\n      1.0\n      0.900391\n      1.0\n      1.0\n      1.0\n      1.0\n      0.206349\n    \n    \n      denom\n      0.857143\n      0.857143\n      0.5\n      0.5\n      4.0\n      4.0\n      32.0\n      32.0\n      1.0\n      1.0\n      2835.0\n      2835.0\n    \n  \n\n\n\n\n\n\nCode\nfig = px.line_polar(gk, r='value', theta='kpi', line_dash='name', color='name', line_close=True\n                    , template=\"plotly_white\", width=800)\n\nfig.update_traces(mode='lines', line_width=5)\nfig.update_layout(legend=dict(x=0.7, y=1.1))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCech was getting around 20% of playing time compared to Leno.\nDistinctive styles: Cech is doing much more high claims while Leno uses more punches.\nCech has a better “saves per minute” ratio, but also - might be the result of him playing much less.\n\n\n\n\n4 most played Defenders\n\n\nCode\ndf = training.query(\"position == 'DF' and Appearances > 18\")[\n    ['clearances_per_min_played', 'fouls_per_min_played', 'yc_per_min_played'\n    ,'tackles_per_min_played','passes_per_min_played', 'Minutes Played', 'assists_per_min_played']\n].rename(columns={'clearances_per_min_played': 'Clearances', 'fouls_per_min_played': 'Fouls'\n                  , 'yc_per_min_played': 'Yellow Cards','tackles_per_min_played': 'Tackles'\n                  ,'passes_per_min_played': 'Passes', 'assists_per_min_played':'Assists'})\n\nmax_map = df.max().to_dict()\ndf = pd.DataFrame(df.unstack()).reset_index()\ndf.columns = ['kpi', 'name', 'value']\ndf['denom'] = df.kpi.map(max_map)\ndf['value'] = df.value / df.denom\ndf.T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n    \n  \n  \n    \n      kpi\n      Clearances\n      Clearances\n      Clearances\n      Clearances\n      Fouls\n      Fouls\n      Fouls\n      Fouls\n      Yellow Cards\n      Yellow Cards\n      Yellow Cards\n      Yellow Cards\n      Tackles\n      Tackles\n      Tackles\n      Tackles\n      Passes\n      Passes\n      Passes\n      Passes\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Assists\n      Assists\n      Assists\n      Assists\n    \n    \n      name\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n      Héctor Bellerín\n      Sead Kolasinac\n      Shkodran Mustafi\n      Sokratis\n    \n    \n      value\n      0.349637\n      0.214844\n      1.0\n      0.797709\n      0.620299\n      0.66036\n      0.954555\n      1.0\n      0.358611\n      0.484788\n      0.630688\n      1.0\n      0.412695\n      0.758747\n      1.0\n      0.805571\n      0.758673\n      0.761175\n      1.0\n      0.934498\n      0.586233\n      0.722753\n      1.0\n      0.840918\n      1.0\n      0.811111\n      0.0\n      0.278854\n    \n    \n      denom\n      0.061568\n      0.061568\n      0.061568\n      0.061568\n      0.016826\n      0.016826\n      0.016826\n      0.016826\n      0.005457\n      0.005457\n      0.005457\n      0.005457\n      0.023709\n      0.023709\n      0.023709\n      0.023709\n      0.674952\n      0.674952\n      0.674952\n      0.674952\n      2615.0\n      2615.0\n      2615.0\n      2615.0\n      0.003262\n      0.003262\n      0.003262\n      0.003262\n    \n  \n\n\n\n\n\n\nCode\nfig = px.line_polar(df, r='value', theta='kpi'\n                    ,color='name', line_dash='name', line_close=True, template=\"plotly_white\")\nfig.update_traces(mode='lines', line_width=4)\nfig.update_layout(legend=dict(x=0, y=1.1))\nfig.show()\n\n\n\n                                                \n\n\n\n\nMustafi leads on most defensive metrics. He also played most minutes from all defenders (3rd overall, after Leno and Aubameyang).\nSokratis has a similar profile, with the exception that he leads by far in yellow cards.\nBellerin and Kolasinac have more offensive profiles, both taking third place in overall team assists rank (5 each, Bellerin played less).\n\n\n\n\nMidfielders\n\n\nCode\nmf = training.query(\"position == 'MF' and Appearances > 20\")[\n    ['assists_per_min_played', 'dispossessed_per_min_played'\n    ,'goals_per_min_played','passes_per_min_played','shots_per_min_played', 'tackles_per_min_played'\n    ,'touches_per_min_played', 'Minutes Played']\n].rename(columns={'assists_per_min_played':'Assists', 'dispossessed_per_min_played':'Dispossessed'\n    ,'goals_per_min_played':'Goals','passes_per_min_played':'Passes','shots_per_min_played':'Shots'\n    , 'tackles_per_min_played':'Tackles','touches_per_min_played':'Touches'})\n\nmax_map = mf.max().to_dict()\n\nmf = pd.DataFrame(mf.unstack()).reset_index()\n\nmf.columns = ['kpi', 'name', 'value']\n\nmf['denom'] = mf.kpi.map(max_map)\n\nmf['value'] = mf.value / mf.denom\nmf.T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n    \n  \n  \n    \n      kpi\n      Assists\n      Assists\n      Assists\n      Assists\n      Assists\n      Assists\n      Assists\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Goals\n      Goals\n      Goals\n      Goals\n      Goals\n      Goals\n      Goals\n      Passes\n      Passes\n      Passes\n      Passes\n      Passes\n      Passes\n      Passes\n      Shots\n      Shots\n      Shots\n      Shots\n      Shots\n      Shots\n      Shots\n      Tackles\n      Tackles\n      Tackles\n      Tackles\n      Tackles\n      Tackles\n      Tackles\n      Touches\n      Touches\n      Touches\n      Touches\n      Touches\n      Touches\n      Touches\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Minutes Played\n      Minutes Played\n    \n    \n      name\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n      Aaron Ramsey\n      Granit Xhaka\n      Henrikh Mkhitaryan\n      Lucas Torreira\n      Matteo Guendouzi\n      Mesut Özil\n      Nacho Monreal\n    \n    \n      value\n      1.0\n      0.176996\n      0.538524\n      0.186073\n      0.0\n      0.25426\n      0.356797\n      1.0\n      0.334326\n      0.74795\n      0.578892\n      0.71183\n      0.960538\n      0.079288\n      0.825301\n      0.438225\n      1.0\n      0.230349\n      0.0\n      0.786904\n      0.147233\n      0.648453\n      1.0\n      0.539397\n      0.744561\n      0.831103\n      0.715386\n      0.699188\n      0.833723\n      0.389036\n      1.0\n      0.352575\n      0.250615\n      0.211982\n      0.126199\n      0.937048\n      0.760976\n      0.623358\n      1.0\n      0.751821\n      0.252269\n      0.68834\n      0.696084\n      1.0\n      0.654089\n      0.776378\n      0.824114\n      0.714137\n      0.762703\n      0.530988\n      1.0\n      0.657337\n      0.95122\n      0.856457\n      0.696122\n      0.744102\n    \n    \n      denom\n      0.004518\n      0.004518\n      0.004518\n      0.004518\n      0.004518\n      0.004518\n      0.004518\n      0.020331\n      0.020331\n      0.020331\n      0.020331\n      0.020331\n      0.020331\n      0.020331\n      0.00365\n      0.00365\n      0.00365\n      0.00365\n      0.00365\n      0.00365\n      0.00365\n      0.897641\n      0.897641\n      0.897641\n      0.897641\n      0.897641\n      0.897641\n      0.897641\n      0.029805\n      0.029805\n      0.029805\n      0.029805\n      0.029805\n      0.029805\n      0.029805\n      0.027322\n      0.027322\n      0.027322\n      0.027322\n      0.027322\n      0.027322\n      0.027322\n      1.113155\n      1.113155\n      1.113155\n      1.113155\n      1.113155\n      1.113155\n      1.113155\n      2501.0\n      2501.0\n      2501.0\n      2501.0\n      2501.0\n      2501.0\n      2501.0\n    \n  \n\n\n\n\n\n\nCode\nfig = px.line_polar(mf, r='value', theta='kpi', color='name', line_dash='name',\n                    line_close=True, template=\"plotly_white\",\n                    width=800) # , color_discrete_sequence= px.colors.sequential.Plasma[-3::-1])\n\nfig.update_traces(mode='lines', line_width=5)\nfig.update_layout(legend=dict(x=1, y=1.1))\nfig.show()\n\n\n\n                                                \n\n\n\n\nThere are two clear patterns on the chart: defensive and offensive. Xhaka and Torreira are representatives of the former, while Mkhitaryan and Özil of the latter.\nRamsey is an interesting exception of a “complete” midfielder here, leading by assists per minute played, but also participating a lot in tackles.\n\n\n\n\nForwards\n\n\nCode\nfw = training.query(\"position == 'FW' and Appearances > 10\")[\n    ['assists_per_min_played', 'missed_chances_per_min_played', 'goals_per_min_played'\n    ,'offsides_per_min_played', 'shots_per_min_played', 'dispossessed_per_min_played'\n     ,'fouls_per_min_played']\n].rename(columns={'assists_per_min_played':'Assists', 'missed_chances_per_min_played':'Missed Chances'\n                 ,'goals_per_min_played':'Goals', 'offsides_per_min_played': 'Offsides'\n                 ,'shots_per_min_played':'Shots', 'dispossessed_per_min_played':'Dispossessed'\n                 ,'fouls_per_min_played':'Fouls'})\n\nmax_map = fw.max().to_dict()\nfw = pd.DataFrame(fw.unstack()).reset_index()\nfw.columns = ['kpi', 'name', 'value']\nfw['denom'] = fw.kpi.map(max_map)\nfw['value'] = fw.value / fw.denom\nfw.T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n    \n  \n  \n    \n      kpi\n      Assists\n      Assists\n      Assists\n      Missed Chances\n      Missed Chances\n      Missed Chances\n      Goals\n      Goals\n      Goals\n      Offsides\n      Offsides\n      Offsides\n      Shots\n      Shots\n      Shots\n      Dispossessed\n      Dispossessed\n      Dispossessed\n      Fouls\n      Fouls\n      Fouls\n    \n    \n      name\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n      Alex Iwobi\n      Alexandre Lacazette\n      Pierre-Emerick Aubameyang\n    \n    \n      value\n      0.952713\n      1.0\n      0.573279\n      0.180638\n      0.616211\n      1.0\n      0.188848\n      0.644221\n      1.0\n      0.105857\n      1.0\n      0.879028\n      0.51565\n      0.939445\n      1.0\n      1.0\n      0.857514\n      0.425511\n      0.19926\n      1.0\n      0.233808\n    \n    \n      denom\n      0.003194\n      0.003194\n      0.003194\n      0.008422\n      0.008422\n      0.008422\n      0.008056\n      0.008056\n      0.008056\n      0.009581\n      0.009581\n      0.009581\n      0.03442\n      0.03442\n      0.03442\n      0.028398\n      0.028398\n      0.028398\n      0.020359\n      0.020359\n      0.020359\n    \n  \n\n\n\n\n\n\nCode\nfig = px.line_polar(fw, r='value', theta='kpi', color='name', line_dash='name',\n                    line_close=True, template=\"plotly_white\",\n                    width=800, text='value')\nfig.update_traces(mode='lines', line_width=5)\nfig.update_layout(legend=dict(x=.8, y=1.1))\nfig.show()\n\n\n\n                                                \n\n\n\n\nLacazette and Aubameyang are 2nd and 4th in most minutes played during the season.\nLacazette fouls five times more than Aubameyang or Iwobi.\nAubameyang leads on goal effectiveness, while Iwobi and Lacazette are great in assists."
  },
  {
    "objectID": "posts/arsenal/arsenal.html#final-thoughts",
    "href": "posts/arsenal/arsenal.html#final-thoughts",
    "title": "Makeover Monday W30: Arsenal FC stats 2018/19",
    "section": "Final thoughts",
    "text": "Final thoughts\nI used canva to assemble the graphs above into the final image, added some red-white colours. I didn’t come up with any breakthroughs, of course. But I had a couple of interesting observations and discoveries during the process. And had some fun too!\nThe greatest benefit was to document the process itself. As Jason Fried urges people to write more to really think the idea through, just sitting down and writing the process of analysis was rewarding."
  },
  {
    "objectID": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html",
    "href": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html",
    "title": "⚖️ Democracy and ⚡ Efficiency",
    "section": "",
    "text": "The AI has already proven to work well for many tasks that were not possible to tackle with computers before. Now we’ve entered the scaling phase to make it:\n\nas accessible as possible (developer tools, explainability, the “democratization of ML”) and\nputting it on as many devices (model efficiency) as possible.\n\nHence, more and more ready-to-use recipes are created, frameworks are hiding complexity, and, pre-built models are optimized and ready to be served on all kind of devices."
  },
  {
    "objectID": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#recommendations",
    "href": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#recommendations",
    "title": "⚖️ Democracy and ⚡ Efficiency",
    "section": "Recommendations",
    "text": "Recommendations\n\nTFRS\nThis one is huge for the RecSys community. Google adds recommendations package into the tensorflow, that makes building, evaluating, and serving sophisticated recommender models easy (this is to the point of democracy).\nThis also involves Maciej Kula, author of a couple of hugely popular reco libraries: LightFM and Spotlight. So it promises to be a very elegant API.\nAnd you can see how easy it is to create even a multitask-system. Here is a code snippet to define two learning tasks, one to predict ratings, another to predict the amount of relevant movies:\ntfrs.tasks.Ranking(\n    loss=tf.keras.losses.MeanSquaredError(),\n    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n)\n\ntfrs.tasks.Retrieval(\n    metrics=tfrs.metrics.FactorizedTopK(\n        candidates=movies.batch(128)\n    )\n)\nThen you can combine these tasks while computing the loss and adjust the weight accordingly. It’s like Lego for recommendations.\n\n\nLinkedIn’s Intents\nA couple of years ago, LinkedIn has joined a cohort of companies that are doing recommendations by intent (members, pages, hashtags, newsletters, etc. in this case) on the main page. Here is a story of how they did it. Some highlights:\n\nUI framework: to be able to quickly switch between recommendation types in the frontend, a unified framework for all the platforms.\n[Micro]-Services: different services for different recommendations with a unified ranker component on top of them. Allows to quickly plug-and-play different algorithms.\nUnified tracking: so often overlooked but such an important mention."
  },
  {
    "objectID": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#efficiency",
    "href": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#efficiency",
    "title": "⚖️ Democracy and ⚡ Efficiency",
    "section": "Efficiency",
    "text": "Efficiency\n\nNVIDIA\nFollowing-up on the last week’s video topic. Building an ML application on top of a video stream is not something easy and requires expertise in multiple domains. So NVIDIA wants to help you make deployment of such kind of applications easier. This also falls into the “democratize” suit. Here is an excerpt from their other article, explaining how to build a real-time face-mask detector application:\n\nTo use TLT [NVIDIA’s transfer learning tool] and DeepStream you do not necessarily have to know all the concepts in depth, such as transfer learning, pruning, quantization, and so on. These simple toolkits abstract away the complexities, allowing you to focus on your application.\n\nSo the modern-day workflow for the AI video app can look like this:\nDownload a pretrained model\n            |\n            |--> Get data for your use case\n                        |\n                        |--> Retrain (Transfer learning) & Prune\n                                    |\n                                    |--> Export model and use with DeepStream library\nI want to point the Prune part, which is becoming more and more relevant for the production systems. And there are many ways to do it, some I’ve covered in a previous post, but you can also check NVIDIA’s blog post.\nWhy is it important? For example, in the face-mask detection example running on a Jetson Nano after pruning the mean average precision has dropped from 86.12 to 85.5%, while frames per second increased more than 3 times — from 6.5 to 21.25.\nThis doesn’t even feel like a trade-off!\nHere is also a free course from them to get started with video analytics: Getting Started with DeepStream for Video Analytics on Jetson Nano.\n\n\nTFLITE’s NLP\nAnd more on the topic of efficiency. Google has added many things around NLP into the TF Lite.\nSo that it’s easier to do things like that in your browser: \nImage from this blog post about the in-browser BERT.\nAnd these capabilities are also unlocked by the pruning and quantization. Just take a look at how much more efficient the model becomes after losing only a fraction in accuracy:"
  },
  {
    "objectID": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#pixar",
    "href": "posts/newsletter/2020-09-28-newsletter-democracy-efficiency.html#pixar",
    "title": "⚖️ Democracy and ⚡ Efficiency",
    "section": "Pixar",
    "text": "Pixar\nAnd after some philosophy let’s end when it’s best. If you always wondered how would you look like a Pixar character, now you have a chance to see that. As well as an informative conversation with its creator.\n\n\nBlending humans and cartoons using @Buntworthy's Google Colab notebook. Thank you for that, it's awesome. Here is a YouTube version of this video: https://t.co/7bUd7nXaX3 pic.twitter.com/iG09lpEAXX\n\n— Doron Adler (@Norod78) August 23, 2020"
  },
  {
    "objectID": "posts/newsletter/2020-08-03-newsletter-data.html",
    "href": "posts/newsletter/2020-08-03-newsletter-data.html",
    "title": "🛢 Data",
    "section": "",
    "text": "Announcements in the metadata management from Shopify and Stripe. What it takes to build a startup in the field of medical AI, and how much time you’ll spend gathering data for it. TensorFlow’s latest release was mostly about its data API."
  },
  {
    "objectID": "posts/newsletter/2020-08-03-newsletter-data.html#ai-in-medicine",
    "href": "posts/newsletter/2020-08-03-newsletter-data.html#ai-in-medicine",
    "title": "🛢 Data",
    "section": "🩺 AI in Medicine",
    "text": "🩺 AI in Medicine\nHighly recommend the Data Futurology podcast about what it takes to build an AI company in the medical sphere. Many interesting things, but what it takes to build real-world datasets in the wild is always worth hearing:\n\n“There is a lot of differences in medical data — if you did MRIs in two different centers, you cannot just take data from both of them and use it.”\n“Another under-appreciated aspect of building a lot of real-world AI applications, where, unlike kaggle, nobody’s got a 100 thousand in a nicely organized folder… Sometimes only having data for 10 patients at a time, scans coming on CDs, 1 at a time.”\n\n\n“As much as our system involves AI and image processing there is probably just as much if not more work in around data standardization, data cleanliness and manual intervention into data.”\n\n\n2.5 years (from 5!) were spent on building a political relationship (with doctors), gathering data piece by piece, later building integrations with existing systems.\n“The best results were coming from building a relationship with individual doctors.”\n\nTo sum it up, I think that data gathering relationship building is the new sales. Building a company that relies on data, you are as good as the number of data providers you’ve built a relationship with."
  },
  {
    "objectID": "posts/newsletter/2020-08-03-newsletter-data.html#metadata",
    "href": "posts/newsletter/2020-08-03-newsletter-data.html#metadata",
    "title": "🛢 Data",
    "section": "📼 Metadata",
    "text": "📼 Metadata\nTwo of the big players have released something about their metadata solutions. Many of the big players already have established solutions for a couple of years, with Shopify being the latest company to build their own.\n\nShopify’s Artifact\n\nTheir implementation uses Elasticseach and a graph database to provide search and data lineage respectively. GraphQL’s Apollo as an API layer. Quite a standard stack. Similar to e.g. this one.\nOther than that, from the screenshots it looks like it does what it should and looks very much like similar systems. However, a quote from the article explaining why it’s generally hard to reuse existing solutions:\n\n\nEvery organization’s data stack is different. While some upstream processes can be standardized and cataloged appropriately, the business context of downstream processes creates a wide distribution of requirements that are near impossible to satisfy with a one-size-fits-all solution.\n\n\n\nStripe and Privacy\nStripe is using their Amundsen metadata tool to increase focus on consumer privacy and better comply with GDPR and CCPA.\n\n\nOther companies\nA collection of data discovery articles."
  },
  {
    "objectID": "posts/newsletter/2020-08-03-newsletter-data.html#tensorflow-2.3",
    "href": "posts/newsletter/2020-08-03-newsletter-data.html#tensorflow-2.3",
    "title": "🛢 Data",
    "section": "🖇 Tensorflow 2.3",
    "text": "🖇 Tensorflow 2.3\nIronically the latest TensorFlow release is also about data. Two of the main additions to the help make preprocessing smoother. I think preprocessing may very well be the most overlooked step and improving it is hugely beneficial.\n\ntd.data.snapshot: allows you to run the preprocessing pipeline once, save the output and play with parameter optimization on top of that. Read more details in the RFC.\nPreprocessing layer API: package preprocessing logic inside a model for easier deployment.\n\n\n\nTo finish on a positive note, here is an awesome 3 minutes Lex Fridman’s video estimating costs for GPT to equal a human brain:\n\n\n\ngpt costs"
  },
  {
    "objectID": "posts/newsletter/2020-10-06-newsletter-data-platforms.html",
    "href": "posts/newsletter/2020-10-06-newsletter-data-platforms.html",
    "title": "📜 History (of Data Platforms and Apps)",
    "section": "",
    "text": "What’s the history of data-intensive applications and how did we end in a state where machines can classify cats better than us?\n\n\nAn interesting take at a hardware-software intersection through the lens of AI applications. Some things that I found particularly fascinating:\n\nThis essay begins by acknowledging a crucial paradox: machine learning researchers mostly ignore hardware despite the role it plays in determining what ideas succeed.\n\n\nComputing started with single-purpose machines.\nThen, in 1969, the general-purpose era began. This meant any move for the application-specific hardware was economically unfeasible because the performance benefit would fade away within 1-2 years with an ever-increasing number of transistors.\n\n\nThe few attempts to deviate and produce specialized supercomputers for research were financially unsustainable and short-lived.\n\n\nHowever, there was a silver lining for specialized hardware. Von Neumann Bottleneck — the available compute is restricted by “the lone channel between the CPU and memory along which data has to travel sequentially”. Hence, in the 2000s GPUs were repurposed to be used with ML applications.\nIn 2012 Google used 16,000 CPU cores to classify cats. In a year, the same task was completed with only 2 CPU cores and 4 GPUs.\nHardware is only economically viable if the lifetime of the use case lasts more than three years. Hence, it already makes sense to build specific hardware for matrix multiplication for quite some time. Next come the unstructured sparsity and weight-specific quantization (what GPU manufacturers are recently doing).\nThe rate of return for additional parameters is decreasing (e.g. Inception V3/ V4: 21.8 vs 41.1 million parameters, 78.8 vs 80 % accuracy).\nThe training costs of GPT-3 is estimated to exceed $12 million dollars.\n\n\n\n\nNo Hadoop, no AWS — barbarian days. Check out the full timeline here, it’s quite fun.\n\n\n\nPre-cloud Timeline\n\n\n\n\n\nAnother article looking at different branches developed in a field of modern data processing. Describes prominent players in areas of data pipelines, catalogs, collaboration and quality.\n\nIn 2010, the number of large enterprises with a Chief Data Officer (CDO) was 15. By 2017, it was up to 4,000. In 2020, it’ll be over 10,000. Why? Data is revenue and revenue is sexy.\n\n\n\n\nYearly state of the AI report. Here are some excerpts from the executive summary (page 7 of the report):\n\nThe hottest area in AI: still NLP\nClosed AI — only 15% of papers publish their code\nBiology starts to benefit from AI (the first AI discovered drug)\nCorporate-driven academic brain drain\nUS and China lead the AI research\nSpecialized hardware investments (see the hardware lottery article above). Semiconductor companies become more and more important.\nTwo wrong arrests using facial recognition."
  },
  {
    "objectID": "posts/newsletter/2020-10-06-newsletter-data-platforms.html#ml-ops",
    "href": "posts/newsletter/2020-10-06-newsletter-data-platforms.html#ml-ops",
    "title": "📜 History (of Data Platforms and Apps)",
    "section": "ML Ops",
    "text": "ML Ops\nOne of the hottest buzzwords in a room. However, I believe, this (and not the mysterious ML platforms) will close the gap in the adoption of ML applications and bring the power of data to the rest of us.\n\nMicrosoft\nMicrosoft continues to do an amazing job for the ML community. Now with the GitHub as well. There is a second part (part 1) of the series related to the ML Ops — what data ops should have become. Integration with github actions is amazing and now supports The Great Expectations action (which is an awesome project in itself). > GitHub Actions don’t just do CI/CD, they provide powerful and flexible automation for ML engineers and data scientists.\n\n\nWhyLogs\nMuch of the difficulty in maintaining an ML system comes from data drift. WhyLogs calculates approximate statistics for datasets of any size up to TB-scale. Available in both Python and Java.\nHere is a data distribution over time from the example walkthrough:\n\n\n\nwhy-logs"
  },
  {
    "objectID": "posts/newsletter/2020-07-28-newsletter-search.html",
    "href": "posts/newsletter/2020-07-28-newsletter-search.html",
    "title": "🔎 Paid Search",
    "section": "",
    "text": "This week I ended up reading a couple of recent articles around the topic of search. Not groundbreaking paper’s style. Rather down-to-earth field implementations. Below, I’ll go through the paid search challenges in two major online platforms. And then to the emerging role of a Relevance Engineer."
  },
  {
    "objectID": "posts/newsletter/2020-07-28-newsletter-search.html#pinterest",
    "href": "posts/newsletter/2020-07-28-newsletter-search.html#pinterest",
    "title": "🔎 Paid Search",
    "section": " Pinterest",
    "text": "Pinterest\nShopping upsells on Pinterest. An interesting story. Let me decompose it to the common steps seen across data projects.\nA simple problem to solve — introduce ads into the search results. They call it “shopping upsells“. Imagine you need to build a shopping upsell model.\n\nStep 1. Get Data.\nWhere to get the data for a feature that doesn’t yet exist on a platform?\n\nOne approach: randomly display a portion of upsells for all queries. However, this way the product quality is mixed with the user intent for shopping — not clear if the user doesn’t want to buy in general or doesn’t like this particular ad.\nA better approach: embed products in both upsell and organic sections, but hide prices in organic. This way is possible to distill the intent of a user and make data less noisy.\n\n\n\nStep 2. Get Model.\nYou’ve got data, get a model.\n\nUse business knowledge to come up with a smart objective. Clicks on products are usually noisy, but a good first start. Much better to assign proper weights to strong signals and smartly combine them. Pinterest uses pins and clicks to partner sites.\nModel architecture:\nQuery -> Embedding -> Encoder -> Dense -> Log Loss\n\nNew practitioners are often disappointed by seeing simple architectures after all the resnets and RNNs they’ve just studied. But complexity and state-of-the-arts are often wrong fallacies to chase for most of the businesses.\n\n\nStep 3a. Get Results.\n\n“After launching the experiment, the model increased more than 2X traffic to the shopping search page without hurting overall search metrics in terms of long clicks or saves. The model also increased more than 2X product impressions and product long clicks through the upsell.“\n\n\n\nStep 3b. Hack Production.\nHaving the results you now need to hack the costs to get the “model economics” right.\n\nFor example, they are smartly precomputing head queries and filtering out “non-shoppable categories, such as ‘recipe’ or ‘finance’.”\n\nMy bet is that Pinterest didn’t come up with these optimizations from the beginning. Usually, it’s a loop of 2-3b steps until you get all the components right. This often-overlooked cycle of small adjustments, in this case, allowed to reduce model serving traffic by 70% 🤯"
  },
  {
    "objectID": "posts/newsletter/2020-07-28-newsletter-search.html#ebay",
    "href": "posts/newsletter/2020-07-28-newsletter-search.html#ebay",
    "title": "🔎 Paid Search",
    "section": " Ebay",
    "text": "Ebay\nEbay’s article on balancing paid and non-paid content in their search results.\nThe basic idea is that having fixed paid slots is bad. Both for the:\n\nhead queries, for which there is much more paid content than it’s possible to fit\nas well as tail queries, for which there is often not enough high-quality paid content\n\nThe solution? Get rid of the fixed paid slots and rank the whole search result according to “relevancy“. Here is a more detailed summary:\n\n\n@ebaytech has recently released an article on balancing the paid and non-paid content in their search result page (thread)⁰#ecommerce #Search #marketplace ⁰https://t.co/REualIf6Aq\n\n— Elias Nema (@EliasNema) July 23, 2020"
  },
  {
    "objectID": "posts/newsletter/2020-07-28-newsletter-search.html#ds-or-ml-re",
    "href": "posts/newsletter/2020-07-28-newsletter-search.html#ds-or-ml-re",
    "title": "🔎 Paid Search",
    "section": "🕵️‍♀️ DS or ML? RE!",
    "text": "🕵️‍♀️ DS or ML? RE!\nAnother interesting take on the career in the data field from one of the most famous search practitioners. A couple of highlights:\n\nWho is a relevance engineer: “implements information retrieval algorithms that solve user information needs in real time, at scale“\nApplied approach: “don’t chase the state of the art unnecessarily, rather they prefer proven techniques for 80% of the problem“, “don’t solve search for Kaggle points or academia, but for real companies and users“\nHow it’s different from ML engineer: both roles are very similar, with relevance engs tending to be more user-centric and focused on IR problems (ML is broader and not necessarily user-facing problems)\n\nI think the role will become more popular going forward with many companies realizing the need and value of showing relevant content to users in an ever-shrinking customer attention span."
  },
  {
    "objectID": "posts/newsletter/2020-07-20-newsletter-gpt.html",
    "href": "posts/newsletter/2020-07-20-newsletter-gpt.html",
    "title": "💥 GPT Excitement and AI Costs",
    "section": "",
    "text": "There were so many tweets, articles and general excitement that it became too much even for Sam Altman himself:\n\n\nThe GPT-3 hype is way too much. It’s impressive (thanks for the nice compliments!) but it still has serious weaknesses and sometimes makes very silly mistakes. AI is going to change the world, but GPT-3 is just a very early glimpse. We have a lot still to figure out.\n\n— Sam Altman (@sama) July 19, 2020\n\n\nI’m, of course, impressed too. In fact, it was able to produce my most liked tweet 😞 (and same happened to some better-known folks):\n\n\nSide note: if this somehow becomes my most viewed tweet ever, I'm going to be sad.\n\n— Leo Polovets (@lpolovets) July 18, 2020\n\n\nThere are many great articles covering GPT-3 and its meaning for the future of humanity. You can easily find them, but probably don’t need to, because they pop up everywhere.\nHype aside, it’s a huge model and probably costs a fortune, but the whole product part is done in a very lean way. It’s at the market validation phase, which it has nailed perfectly.\n\n\nIn the ultimate lean startup twist it turned out that @sama was manually answering all GPT-3 requests.\n\n— Andreas Klinger ✌️ (@andreasklinger) July 17, 2020\n\n\nProbably, the next step would be to optimize costs and make the economics right for the rest of the internet. Which brings us to the topic of costs, which becomes as relevant as never for the AI/ML community."
  },
  {
    "objectID": "posts/newsletter/2020-07-20-newsletter-gpt.html#costs-in-ai",
    "href": "posts/newsletter/2020-07-20-newsletter-gpt.html#costs-in-ai",
    "title": "💥 GPT Excitement and AI Costs",
    "section": "💰Costs in AI",
    "text": "💰Costs in AI\n\nA very interesting conversation on last week’s TWIML AI podcast about model design optimization for hardware.\nAnother article suggests that despite shaving off 3/4 of errors in logistic optimization prediction with the help of deep learning, a European retailer chose not to use the model because of costs.\n\nUntil very recently, DL has been driven by the research in big companies. This means almost unlimited resources. It’s great to validate and/or win the market. But with time, you need to get the unit economics right. For training and serving and smaller devices (a smartphone in 2017 was able to run AlexNet only for 1 hour).\nBasically there are multiple directions in ML optimizations:\n\nIncorporating power/energy/latency constraints into network architectures search. This “can bring 5‐10x improvement in energy or latency with minimal loss in accuracy or can satisfy real-time constraints for inference”. Basically, by thinking about hardware constraints in advance you can get to almost the same accuracy while saving in the order of magnitudes. An amazing trade-off for most of the businesses.\nQuantizing neural networks. The idea here is to round model weights to the nearest power of 2, hence allowing using shift and add operations to replace the multiplications. This improves speed and lowers energy consumption. A very smart approach and again, a well-worse trade-off.\nEnergy-aware pruning of NNs: Often both accuracy and latency are important to the application. This work allows you to quickly iterate over accuracy-vs.-speed trade-off for finding a sweet-spot for a particular application using model compression.\nDiscretizing vectors over a d-dimensional sphere: A super-smart approach where instead of adapting an index to the data, the data is adopted to the index itself. “We learn a neural network that aims at preserving the neighborhood structure in the input space while best covering the output space (uniformly)”.\n\n\n\n\nto uniform\n\n\nThese are just the directions I’ve seen recently. But the topic is becoming more and more important. If AI aims to turn into a new cloud, the industry needs to figure out the ways to scale the “state-of-the-art“ to the rest of the internet. And it looks like we are finally getting there."
  },
  {
    "objectID": "posts/newsletter/2020-07-13-newsletter-no-code.html",
    "href": "posts/newsletter/2020-07-13-newsletter-no-code.html",
    "title": "📵 No Code and ML Interns",
    "section": "",
    "text": "Is no code the new oil or the old ETL? TikTok’s recommender and when to use machine learning."
  },
  {
    "objectID": "posts/newsletter/2020-07-13-newsletter-no-code.html#no-code",
    "href": "posts/newsletter/2020-07-13-newsletter-no-code.html#no-code",
    "title": "📵 No Code and ML Interns",
    "section": "📵 No Code",
    "text": "📵 No Code\n\nWhen AWS launches a service, there is usually good economics behind it. Recently they’ve launched the no-code way to build simple mobile apps. It continues the trend started by Notion, Airtable and the likes of building your own productivity tools (of course, AWS is not competing against the Notion but rather against Microsoft Power Apps here). Spreadsheets for the new era.\nThe Russian design studio was using AI as one of their designers for more than a year. > ”This event marks the moment that the mass automation of creative processes becomes a reality for businesses.”\nMonkeyLearn raises 2.2M$ - not really no-code AI, rather NLP as a service. > “Our vision is to make AI approachable by providing a toolkit for teams to actually use AI in their daily operations,” Garreta said in a release.\nAdobe keeps investing in its Sensei AI platform adding seamless no code personalization and recommenders based on AL and ML, of course. Allows you to automatically A/B test different content variants and more.\n\n\n\n\nAutomatically Testing Multiple Onboarding Flows with Adobe Sensei"
  },
  {
    "objectID": "posts/newsletter/2020-07-13-newsletter-no-code.html#ml-applications",
    "href": "posts/newsletter/2020-07-13-newsletter-no-code.html#ml-applications",
    "title": "📵 No Code and ML Interns",
    "section": "🎛 ML Applications",
    "text": "🎛 ML Applications\n\nFacebook will prioritize original reporting in the home feed. To identify originals it will look at how often the article is cited as … original.\nA nice launch on ProductHunt using GANs to generate models (real ones) for “photoshoots“. Even though the demo is far from perfect. But with the recent e-commerce rise, covid restrictions and all the shopify storefronts, this might be a promising direction. If the model (ML model, in this case) economy can be figured out.\nTikTok’s recommenders are apparently really good:\n\n\n\nAccording to my teenagers, Tiktok has by far the best personalized #RecSys they have used. Do we know anything about what they are doing?\n\n— Xavier Amatriain - 🌈💪🏿 (@xamat) July 13, 2020\n\n\nAnd my thread about their recent article about how their recommender works:\n\n\n#Tiktok has posted an article about their #recommendations system. There aren't too many details, but some interesting quotes on how they think about the problem space.(1/)https://t.co/Ou8LfBGjUe\n\n— Elias Nema (@EliasNema) July 2, 2020"
  },
  {
    "objectID": "posts/newsletter/2020-07-13-newsletter-no-code.html#why-ml-is-similar-to-interns",
    "href": "posts/newsletter/2020-07-13-newsletter-no-code.html#why-ml-is-similar-to-interns",
    "title": "📵 No Code and ML Interns",
    "section": "👔 Why ML is Similar to Interns",
    "text": "👔 Why ML is Similar to Interns\nFinishing with an interesting tweet from Benedict Evans:\n\n\nI quite often describe machine learning as giving you infinite interns. This can be a useful way to look at how ML would affect a product: sometimes having infinite free interns to look at data for you wouldn’t actually solve your problems, and the struggle is something else.\n\n— Benedict Evans (@benedictevans) July 6, 2020\n\n\nI think this is a very good summary for execs who don’t have exposure to the topic. Or for product managers when thinking about prioritization. Very often people tend to forget that ML is about automation. Imagine, you have an amazing photo-artist, who can look at the photos you take, pick the best ones, adjust brightness here and there, suggest collages, design a photo book and send to you. This is an awesome service, but also a very expensive one. How about providing it to a billion people? Hence, Google Photos cannot compete with the quality of the artist but can provide the service to a billion customers. Same goes for recommendations: a good stylist can probably get you a fashionable outfit, but economics cannot beat the recsys approach (or it can if you are a boutique).\nOn the other hand, problems where many agents won’t help, such as: shall we launch in the new market, how will customers react to a new feature? These are non-scalable problems. And even if you can and should use data to make a conscious decision there, investing in the ML solution won’t bring benefits.\nToo often people confuse the need of scaling the decision (good for ML interns) with the need of taking a good decision from time to time (ML is of no use here)."
  },
  {
    "objectID": "posts/newsletter/2020-09-22-newsletter-video.html",
    "href": "posts/newsletter/2020-09-22-newsletter-video.html",
    "title": "📹 Tensorflow.js, AI in Video and Analytics",
    "section": "",
    "text": "What can you do with ML in a modern browser? A showcase from the TensorFlow.js community. My personal favorites were:\n\nTouch-less interface for your hand. It takes some time to get used to it and there is still some polishing to be made. However, after a bit of practice, it becomes kind of fun.\nAnalyze emotions of your audience in real-time, so that your amazing jokes no longer end in awkwardly muted silence."
  },
  {
    "objectID": "posts/newsletter/2020-09-22-newsletter-video.html#ai-in-video",
    "href": "posts/newsletter/2020-09-22-newsletter-video.html#ai-in-video",
    "title": "📹 Tensorflow.js, AI in Video and Analytics",
    "section": "AI in Video",
    "text": "AI in Video\n\nSynthesia — a new service to generate video-content:\n\nChose from predefined narrators\nType the script\nIt’ll create a video of the person presenting your text in some minutes\n\nNorfair — an open-source library from the Tyro-labs for the object tracking (cars, pedestrians, poses).\nAnd if those are not cool enough for you, how about generating realistic tennis matches with real players. Vid2Player does exactly that. Wanted to change the grand-slam history or play Federer against Federer? Well, now you can do that:\n\n\n\n\nAnd since both AI and Video require quite some compute resources and following the horrible launch of 30x cards from NVIDIA, here is a guide on how to chose the one that suits you best while waiting for the cards’ availability."
  },
  {
    "objectID": "posts/newsletter/2020-09-22-newsletter-video.html#analytics",
    "href": "posts/newsletter/2020-09-22-newsletter-video.html#analytics",
    "title": "📹 Tensorflow.js, AI in Video and Analytics",
    "section": "Analytics",
    "text": "Analytics\nRelaunch of the analytics blog at Netflix has brought two recent articles. The first one is about the broader role of an analyst. I think this diagram is quite cool and shows the depth of what’s analytics in data organizations.\n\n\n\nanalytics-netflix\n\n\nIn the other article from them, there is an interview with a couple of data folks. In the spirit of:\n\nEveryone wants to build fancy models or tools, but fewer are willing to do the foundational things like cleaning the data and writing the documentation.\n\nEnough of Netflix. Lastly, an interesting (though quite wordy) take on data cleansing and why it’s not as simple as it’s often presented. I enjoy a lot the attitude from the author:\n\nTL;DR: Cleaning data is considered by some people [citation needed] to be menial work that’s somehow “beneath” the sexy “real” data science work. I call BS."
  },
  {
    "objectID": "posts/structured-sql/structured-sql.html",
    "href": "posts/structured-sql/structured-sql.html",
    "title": "Getting “Structured” Back to SQL",
    "section": "",
    "text": "Structure [d Query Language]\nBut how to create a structure? First, begin with the end in mind. What should the answer look like? For example, you want to analyze revenue for a specific sales channel by different region. See, it’s already a prepared SELECT statement.\nHere and below, I’ll be using pseudo-SQL to avoid unrelated details.\nSELECT channel, region, SUM(sales)\nUsually, there would be the main subject in your question. Above, you want to analyze revenue. So sales is going to be your main entity, a driving table. In FROM, you should always put it first.\nFROM sales                                 <---- driving table\nNow you want to filter for a specific channel. For this, go to the new table - channels. When adding it, think of your query as a tree - the main table as a body and the new table as a branch.\nFROM sales                                 <---- driving table\n\n  JOIN channels ON channel = 'web'         <---- branch 1\n\nOne thing to keep in mind when adding tables is the granularity you are operating on. The last thing you want is to introduce a row explosion when joining. Write your join conditions carefully.\n\nThe next step is to group results by region. In the sales table, there is only a district_id. For a region you need to go to districts -> cities -> regions tables. Hear your branch would consist of multiple tables.\n  JOIN districts                           <---- branch 2.1\n    JOIN cities                            <---- branch 2.2\n      JOIN regions                         <---- branch 2.3\nAnother way to think about this: I need to get the name of a region for each district. Basically, district_id, region_name with the join condition on district_id. It’s always good to double-check the join condition with a simple query like:\nSELECT district_id, count(*)\nFROM (district_region_branch)   <---  branch 2 from the previous query\nGROUP BY district_id\nHAVING count(*) > 1\nThis will check if you have duplicates on the join key. Ideally, you will do this check for every branch introduced.\nBranching metaphor also helps with rules for OUTER joins - whenever introduced, carry it over for all the join conditions till the end of the current branch. If there are data inconsistencies and some cities are not available in the city table. Hence you introduce a LEFT JOIN and carry it:\n  JOIN districts                           <---- branch\n    LEFT JOIN cities                       <---- if outer join here\n      LEFT JOIN regions                    <---- then also here\nOk, so what do we have at the end?\nSELECT region, SUM(sales)\nFROM sales                                 <---- driving table\n\n  JOIN channels WHERE channel = 'web'      <---- branch 1\n\n  JOIN districts                           <---- branch 2.1\n    LEFT JOIN cities                       <---- branch 2.2\n      LEFT JOIN regions                    <---- branch 2.3\n\nGROUP BY region\nNote how indentation makes the query structure more readable and puts all tables in purposeful groups.\n\n\nConclusion and a recipe\nOf course, we looked at quite a simple query. And SQL is sophisticated nowadays. You can do a JSON, pattern recognition, complex aggregations. However, the structure should come first. Here is how to enforce it:\n\n\nBegin with the end in mind. Think about how your answer should look like.\nFind the main subject. Always put it to a FROM first. If there is more than one - wrap each into a CTE and apply these steps to each of them.\nAdd tables to the main one intent at a time. E.g.: “all the following JOINs are here to get a region for a sale”.\nBe very careful about your joins. Ensure the table you add has not more than one row per join condition.\nMove to grouping, analytical functions, etc. only after you’ve finished connecting all the data sources.\n\n\nOnce you have learned how to get the data you need from different sources and documented it in the form of a readable structure, the query will tell a story of your analysis in itself. More importantly, it will help others to better understand your intents and trust your results."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Building Data-Intensive Teams @ DataTalks.Club Conference: Product and Process\n\n\n\n\n\n Building Data-Intensive Teams  from Elias Nema\n\n\n\nWalking Around Your Nearest Neighbors @ Berlin Buzzwords | MICES | Haystack 2020\n\n\n\n\n\n Walking Around Your Nearest Neighbors with Lambda Architecture  from Elias Nema\n\n\n\nHypothesize your Way into Better Product Development @ Berlin Buzzwords | MICES | Haystack 2020\n\n\n\n\n\n Hypothesize your Way into Better Product Development  from Elias Nema"
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "PRIVACY",
    "section": "",
    "text": "The Flaguiz app stores data about your learning progress locally on the device. The data is not submitted to the internet and not shared with any 3rd parties. The app doesn’t collect or send any personal information.\nIf you have any questions or suggestions, do not hesitate to contact me at Elias Nema, 19 Dunluce, D04 E6V4 Dublin, Ireland, me@eliasnema.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data meets product",
    "section": "",
    "text": "Elias Nema\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElias Nema\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Things I’m also working on:\n\nA book, called “Hypothesize!”, about better ways of managing data products. You can pre-order it here at half price.\nAn occasional newsletter - “Data Meets Product” - focused on news about data applications.\nHere are some of my talks."
  }
]